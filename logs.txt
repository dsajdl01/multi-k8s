* 
* ==> Audit <==
* |------------|------|----------|-------|---------|-------------------------------|-------------------------------|
|  Command   | Args | Profile  | User  | Version |          Start Time           |           End Time            |
|------------|------|----------|-------|---------|-------------------------------|-------------------------------|
| start      |      | minikube | david | v1.24.0 | Sun, 16 Jan 2022 12:50:05 GMT | Sun, 16 Jan 2022 12:53:51 GMT |
| start      |      | minikube | david | v1.24.0 | Sun, 16 Jan 2022 12:54:35 GMT | Sun, 16 Jan 2022 12:54:37 GMT |
| start      |      | minikube | david | v1.24.0 | Wed, 19 Jan 2022 19:18:07 GMT | Wed, 19 Jan 2022 19:18:27 GMT |
| ip         |      | minikube | david | v1.24.0 | Wed, 19 Jan 2022 19:24:12 GMT | Wed, 19 Jan 2022 19:24:12 GMT |
| start      |      | minikube | david | v1.24.0 | Fri, 21 Jan 2022 17:43:40 GMT | Fri, 21 Jan 2022 17:44:02 GMT |
| start      |      | minikube | david | v1.24.0 | Fri, 21 Jan 2022 17:47:59 GMT | Fri, 21 Jan 2022 17:48:22 GMT |
| start      |      | minikube | david | v1.24.0 | Fri, 21 Jan 2022 17:49:20 GMT | Fri, 21 Jan 2022 17:49:23 GMT |
| ip         |      | minikube | david | v1.24.0 | Fri, 21 Jan 2022 17:49:57 GMT | Fri, 21 Jan 2022 17:49:57 GMT |
| start      |      | minikube | david | v1.24.0 | Fri, 21 Jan 2022 17:51:21 GMT | Fri, 21 Jan 2022 17:51:24 GMT |
| ip         |      | minikube | david | v1.24.0 | Fri, 21 Jan 2022 17:51:29 GMT | Fri, 21 Jan 2022 17:51:29 GMT |
| start      |      | minikube | david | v1.24.0 | Mon, 24 Jan 2022 17:37:56 GMT | Mon, 24 Jan 2022 17:38:15 GMT |
| ip         |      | minikube | david | v1.24.0 | Mon, 24 Jan 2022 18:51:33 GMT | Mon, 24 Jan 2022 18:51:33 GMT |
| start      |      | minikube | david | v1.24.0 | Wed, 26 Jan 2022 17:31:08 GMT | Wed, 26 Jan 2022 17:31:29 GMT |
| ip         |      | minikube | david | v1.24.0 | Wed, 26 Jan 2022 18:09:56 GMT | Wed, 26 Jan 2022 18:09:56 GMT |
| stop       |      | minikube | david | v1.24.0 | Wed, 26 Jan 2022 19:17:10 GMT | Wed, 26 Jan 2022 19:17:20 GMT |
| stop       |      | minikube | david | v1.24.0 | Wed, 26 Jan 2022 19:17:45 GMT | Wed, 26 Jan 2022 19:17:46 GMT |
| stop       |      | minikube | david | v1.24.0 | Wed, 26 Jan 2022 19:17:52 GMT | Wed, 26 Jan 2022 19:17:52 GMT |
| start      |      | minikube | david | v1.24.0 | Fri, 28 Jan 2022 16:41:42 GMT | Fri, 28 Jan 2022 16:42:01 GMT |
| ip         |      | minikube | david | v1.24.0 | Fri, 28 Jan 2022 16:42:12 GMT | Fri, 28 Jan 2022 16:42:12 GMT |
| docker-env |      | minikube | david | v1.24.0 | Fri, 28 Jan 2022 17:23:41 GMT | Fri, 28 Jan 2022 17:23:41 GMT |
| docker-env |      | minikube | david | v1.24.0 | Fri, 28 Jan 2022 17:25:15 GMT | Fri, 28 Jan 2022 17:25:16 GMT |
| ip         |      | minikube | david | v1.24.0 | Fri, 28 Jan 2022 17:37:46 GMT | Fri, 28 Jan 2022 17:37:46 GMT |
| start      |      | minikube | david | v1.24.0 | Mon, 31 Jan 2022 17:31:15 GMT | Mon, 31 Jan 2022 17:31:38 GMT |
| stop       |      | minikube | david | v1.24.0 | Mon, 31 Jan 2022 19:13:48 GMT | Mon, 31 Jan 2022 19:14:00 GMT |
| start      |      | minikube | david | v1.24.0 | Tue, 01 Feb 2022 14:55:20 GMT | Tue, 01 Feb 2022 14:55:38 GMT |
| stop       |      | minikube | david | v1.24.0 | Tue, 01 Feb 2022 16:51:04 GMT | Tue, 01 Feb 2022 16:51:15 GMT |
| start      |      | minikube | david | v1.24.0 | Mon, 07 Feb 2022 18:53:05 GMT | Mon, 07 Feb 2022 18:53:25 GMT |
| start      |      | minikube | david | v1.24.0 | Wed, 09 Feb 2022 18:20:42 GMT | Wed, 09 Feb 2022 18:21:02 GMT |
| stop       |      | minikube | david | v1.24.0 | Wed, 09 Feb 2022 18:55:07 GMT | Wed, 09 Feb 2022 18:55:19 GMT |
| start      |      | minikube | david | v1.24.0 | Fri, 11 Feb 2022 18:10:16 GMT | Fri, 11 Feb 2022 18:10:36 GMT |
|------------|------|----------|-------|---------|-------------------------------|-------------------------------|

* 
* ==> Last Start <==
* Log file created at: 2022/02/11 18:10:16
Running on machine: david-XPS-15-9500
Binary: Built with gc go1.17.2 for linux/amd64
Log line format: [IWEF]mmdd hh:mm:ss.uuuuuu threadid file:line] msg
I0211 18:10:16.631551  216445 out.go:297] Setting OutFile to fd 1 ...
I0211 18:10:16.631880  216445 out.go:349] isatty.IsTerminal(1) = true
I0211 18:10:16.631883  216445 out.go:310] Setting ErrFile to fd 2...
I0211 18:10:16.631887  216445 out.go:349] isatty.IsTerminal(2) = true
I0211 18:10:16.631955  216445 root.go:313] Updating PATH: /home/david/.minikube/bin
W0211 18:10:16.632127  216445 root.go:291] Error reading config file at /home/david/.minikube/config/config.json: open /home/david/.minikube/config/config.json: no such file or directory
I0211 18:10:16.632433  216445 out.go:304] Setting JSON to false
I0211 18:10:16.654473  216445 start.go:112] hostinfo: {"hostname":"david-XPS-15-9500","uptime":26448,"bootTime":1644576568,"procs":437,"os":"linux","platform":"ubuntu","platformFamily":"debian","platformVersion":"20.04","kernelVersion":"5.13.0-28-generic","kernelArch":"x86_64","virtualizationSystem":"kvm","virtualizationRole":"host","hostId":"64dae035-5f66-4caf-9e0f-4a41a80bab0c"}
I0211 18:10:16.654520  216445 start.go:122] virtualization: kvm host
I0211 18:10:16.656543  216445 out.go:176] 😄  minikube v1.24.0 on Ubuntu 20.04
I0211 18:10:16.657086  216445 notify.go:174] Checking for updates...
I0211 18:10:16.657734  216445 config.go:176] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.22.3
I0211 18:10:16.658447  216445 driver.go:343] Setting default libvirt URI to qemu:///system
I0211 18:10:16.768727  216445 docker.go:132] docker version: linux-20.10.12
I0211 18:10:16.769202  216445 cli_runner.go:115] Run: docker system info --format "{{json .}}"
I0211 18:10:16.771627  216445 lock.go:35] WriteFile acquiring /home/david/.minikube/last_update_check: {Name:mk548624ae3dcee89025bb4492b2d4914f8e3893 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0211 18:10:16.774273  216445 out.go:176] 🎉  minikube 1.25.1 is available! Download it: https://github.com/kubernetes/minikube/releases/tag/v1.25.1
I0211 18:10:16.775525  216445 out.go:176] 💡  To disable this notice, run: 'minikube config set WantUpdateNotification false'

I0211 18:10:16.997681  216445 info.go:263] docker info: {ID:XW76:OKTQ:GZGC:O4N5:45IV:2BGR:DNRY:5VEP:ZFEX:5L6F:SISO:55GZ Containers:78 ContainersRunning:0 ContainersPaused:0 ContainersStopped:78 Images:158 Driver:overlay2 DriverStatus:[[Backing Filesystem extfs] [Supports d_type true] [Native Overlay Diff true] [userxattr false]] SystemStatus:<nil> Plugins:{Volume:[local] Network:[bridge host ipvlan macvlan null overlay] Authorization:<nil> Log:[awslogs fluentd gcplogs gelf journald json-file local logentries splunk syslog]} MemoryLimit:true SwapLimit:true KernelMemory:true KernelMemoryTCP:true CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:true IPv4Forwarding:true BridgeNfIptables:true BridgeNfIP6Tables:true Debug:false NFd:24 OomKillDisable:true NGoroutines:36 SystemTime:2022-02-11 18:10:16.788637334 +0000 UTC LoggingDriver:json-file CgroupDriver:cgroupfs NEventsListener:0 KernelVersion:5.13.0-28-generic OperatingSystem:Ubuntu 20.04.3 LTS OSType:linux Architecture:x86_64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:{AllowNondistributableArtifactsCIDRs:[] AllowNondistributableArtifactsHostnames:[] InsecureRegistryCIDRs:[127.0.0.0/8] IndexConfigs:{DockerIo:{Name:docker.io Mirrors:[] Secure:true Official:true}} Mirrors:[]} NCPU:12 MemTotal:16380239872 GenericResources:<nil> DockerRootDir:/var/lib/docker HTTPProxy: HTTPSProxy: NoProxy: Name:david-XPS-15-9500 Labels:[] ExperimentalBuild:false ServerVersion:20.10.12 ClusterStore: ClusterAdvertise: Runtimes:{Runc:{Path:runc}} DefaultRuntime:runc Swarm:{NodeID: NodeAddr: LocalNodeState:inactive ControlAvailable:false Error: RemoteManagers:<nil>} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID:7b11cfaabd73bb80907dd23182b9347b4245eb5d Expected:7b11cfaabd73bb80907dd23182b9347b4245eb5d} RuncCommit:{ID:v1.0.2-0-g52b36a2 Expected:v1.0.2-0-g52b36a2} InitCommit:{ID:de40ad0 Expected:de40ad0} SecurityOptions:[name=apparmor name=seccomp,profile=default] ProductLicense: Warnings:<nil> ServerErrors:[] ClientInfo:{Debug:false Plugins:[map[Experimental:true Name:app Path:/usr/libexec/docker/cli-plugins/docker-app SchemaVersion:0.1.0 ShortDescription:Docker App Vendor:Docker Inc. Version:v0.9.1-beta3] map[Name:buildx Path:/usr/libexec/docker/cli-plugins/docker-buildx SchemaVersion:0.1.0 ShortDescription:Docker Buildx Vendor:Docker Inc. Version:v0.7.1-docker] map[Name:scan Path:/usr/libexec/docker/cli-plugins/docker-scan SchemaVersion:0.1.0 ShortDescription:Docker Scan Vendor:Docker Inc. Version:v0.12.0]] Warnings:<nil>}}
I0211 18:10:16.997758  216445 docker.go:237] overlay module found
I0211 18:10:16.999407  216445 out.go:176] ✨  Using the docker driver based on existing profile
I0211 18:10:16.999420  216445 start.go:280] selected driver: docker
I0211 18:10:16.999422  216445 start.go:762] validating driver "docker" against &{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.28@sha256:4780f1897569d2bf77aafb3d133a08d42b4fe61127f06fcfc90c2c5d902d893c Memory:3900 CPUs:2 DiskSize:20000 VMDriver: Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.22.3 ClusterName:minikube Namespace:default APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin: FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI: NodeIP: NodePort:8443 NodeName:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.22.3 ControlPlane:true Worker:true}] Addons:map[ambassador:false auto-pause:false csi-hostpath-driver:false dashboard:false default-storageclass:true efk:false freshpod:false gcp-auth:false gvisor:false helm-tiller:false ingress:false ingress-dns:false istio:false istio-provisioner:false kubevirt:false logviewer:false metallb:false metrics-server:false nvidia-driver-installer:false nvidia-gpu-device-plugin:false olm:false pod-security-policy:false portainer:false registry:false registry-aliases:false registry-creds:false storage-provisioner:true storage-provisioner-gluster:false volumesnapshots:false] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/home/david:/minikube-host}
I0211 18:10:16.999478  216445 start.go:773] status for docker: {Installed:true Healthy:true Running:false NeedsImprovement:false Error:<nil> Reason: Fix: Doc:}
I0211 18:10:16.999587  216445 cli_runner.go:115] Run: docker system info --format "{{json .}}"
I0211 18:10:17.065459  216445 info.go:263] docker info: {ID:XW76:OKTQ:GZGC:O4N5:45IV:2BGR:DNRY:5VEP:ZFEX:5L6F:SISO:55GZ Containers:78 ContainersRunning:0 ContainersPaused:0 ContainersStopped:78 Images:158 Driver:overlay2 DriverStatus:[[Backing Filesystem extfs] [Supports d_type true] [Native Overlay Diff true] [userxattr false]] SystemStatus:<nil> Plugins:{Volume:[local] Network:[bridge host ipvlan macvlan null overlay] Authorization:<nil> Log:[awslogs fluentd gcplogs gelf journald json-file local logentries splunk syslog]} MemoryLimit:true SwapLimit:true KernelMemory:true KernelMemoryTCP:true CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:true IPv4Forwarding:true BridgeNfIptables:true BridgeNfIP6Tables:true Debug:false NFd:24 OomKillDisable:true NGoroutines:36 SystemTime:2022-02-11 18:10:17.020232265 +0000 UTC LoggingDriver:json-file CgroupDriver:cgroupfs NEventsListener:0 KernelVersion:5.13.0-28-generic OperatingSystem:Ubuntu 20.04.3 LTS OSType:linux Architecture:x86_64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:{AllowNondistributableArtifactsCIDRs:[] AllowNondistributableArtifactsHostnames:[] InsecureRegistryCIDRs:[127.0.0.0/8] IndexConfigs:{DockerIo:{Name:docker.io Mirrors:[] Secure:true Official:true}} Mirrors:[]} NCPU:12 MemTotal:16380239872 GenericResources:<nil> DockerRootDir:/var/lib/docker HTTPProxy: HTTPSProxy: NoProxy: Name:david-XPS-15-9500 Labels:[] ExperimentalBuild:false ServerVersion:20.10.12 ClusterStore: ClusterAdvertise: Runtimes:{Runc:{Path:runc}} DefaultRuntime:runc Swarm:{NodeID: NodeAddr: LocalNodeState:inactive ControlAvailable:false Error: RemoteManagers:<nil>} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID:7b11cfaabd73bb80907dd23182b9347b4245eb5d Expected:7b11cfaabd73bb80907dd23182b9347b4245eb5d} RuncCommit:{ID:v1.0.2-0-g52b36a2 Expected:v1.0.2-0-g52b36a2} InitCommit:{ID:de40ad0 Expected:de40ad0} SecurityOptions:[name=apparmor name=seccomp,profile=default] ProductLicense: Warnings:<nil> ServerErrors:[] ClientInfo:{Debug:false Plugins:[map[Experimental:true Name:app Path:/usr/libexec/docker/cli-plugins/docker-app SchemaVersion:0.1.0 ShortDescription:Docker App Vendor:Docker Inc. Version:v0.9.1-beta3] map[Name:buildx Path:/usr/libexec/docker/cli-plugins/docker-buildx SchemaVersion:0.1.0 ShortDescription:Docker Buildx Vendor:Docker Inc. Version:v0.7.1-docker] map[Name:scan Path:/usr/libexec/docker/cli-plugins/docker-scan SchemaVersion:0.1.0 ShortDescription:Docker Scan Vendor:Docker Inc. Version:v0.12.0]] Warnings:<nil>}}
I0211 18:10:17.086104  216445 cni.go:93] Creating CNI manager for ""
I0211 18:10:17.086517  216445 cni.go:167] CNI unnecessary in this configuration, recommending no CNI
I0211 18:10:17.086521  216445 start_flags.go:282] config:
{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.28@sha256:4780f1897569d2bf77aafb3d133a08d42b4fe61127f06fcfc90c2c5d902d893c Memory:3900 CPUs:2 DiskSize:20000 VMDriver: Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.22.3 ClusterName:minikube Namespace:default APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin: FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI: NodeIP: NodePort:8443 NodeName:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.22.3 ControlPlane:true Worker:true}] Addons:map[ambassador:false auto-pause:false csi-hostpath-driver:false dashboard:false default-storageclass:true efk:false freshpod:false gcp-auth:false gvisor:false helm-tiller:false ingress:false ingress-dns:false istio:false istio-provisioner:false kubevirt:false logviewer:false metallb:false metrics-server:false nvidia-driver-installer:false nvidia-gpu-device-plugin:false olm:false pod-security-policy:false portainer:false registry:false registry-aliases:false registry-creds:false storage-provisioner:true storage-provisioner-gluster:false volumesnapshots:false] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/home/david:/minikube-host}
I0211 18:10:17.088302  216445 out.go:176] 👍  Starting control plane node minikube in cluster minikube
I0211 18:10:17.088540  216445 cache.go:118] Beginning downloading kic base image for docker with docker
I0211 18:10:17.089864  216445 out.go:176] 🚜  Pulling base image ...
I0211 18:10:17.089880  216445 preload.go:132] Checking if preload exists for k8s version v1.22.3 and runtime docker
I0211 18:10:17.089906  216445 image.go:75] Checking for gcr.io/k8s-minikube/kicbase:v0.0.28@sha256:4780f1897569d2bf77aafb3d133a08d42b4fe61127f06fcfc90c2c5d902d893c in local docker daemon
I0211 18:10:17.089909  216445 preload.go:148] Found local preload: /home/david/.minikube/cache/preloaded-tarball/preloaded-images-k8s-v13-v1.22.3-docker-overlay2-amd64.tar.lz4
I0211 18:10:17.089912  216445 cache.go:57] Caching tarball of preloaded images
I0211 18:10:17.089997  216445 preload.go:174] Found /home/david/.minikube/cache/preloaded-tarball/preloaded-images-k8s-v13-v1.22.3-docker-overlay2-amd64.tar.lz4 in cache, skipping download
I0211 18:10:17.090007  216445 cache.go:60] Finished verifying existence of preloaded tar for  v1.22.3 on docker
I0211 18:10:17.090091  216445 profile.go:147] Saving config to /home/david/.minikube/profiles/minikube/config.json ...
I0211 18:10:17.198221  216445 image.go:79] Found gcr.io/k8s-minikube/kicbase:v0.0.28@sha256:4780f1897569d2bf77aafb3d133a08d42b4fe61127f06fcfc90c2c5d902d893c in local docker daemon, skipping pull
I0211 18:10:17.198470  216445 cache.go:140] gcr.io/k8s-minikube/kicbase:v0.0.28@sha256:4780f1897569d2bf77aafb3d133a08d42b4fe61127f06fcfc90c2c5d902d893c exists in daemon, skipping load
I0211 18:10:17.198479  216445 cache.go:206] Successfully downloaded all kic artifacts
I0211 18:10:17.198499  216445 start.go:313] acquiring machines lock for minikube: {Name:mk22acfb2099d2ba5f8cdfaddb20ce3bd07e8eb7 Clock:{} Delay:500ms Timeout:10m0s Cancel:<nil>}
I0211 18:10:17.198573  216445 start.go:317] acquired machines lock for "minikube" in 64.576µs
I0211 18:10:17.198584  216445 start.go:93] Skipping create...Using existing machine configuration
I0211 18:10:17.198587  216445 fix.go:55] fixHost starting: 
I0211 18:10:17.198740  216445 cli_runner.go:115] Run: docker container inspect minikube --format={{.State.Status}}
I0211 18:10:17.225099  216445 fix.go:108] recreateIfNeeded on minikube: state=Stopped err=<nil>
W0211 18:10:17.225115  216445 fix.go:134] unexpected machine state, will restart: <nil>
I0211 18:10:17.227983  216445 out.go:176] 🔄  Restarting existing docker container for "minikube" ...
I0211 18:10:17.228019  216445 cli_runner.go:115] Run: docker start minikube
I0211 18:10:17.636696  216445 cli_runner.go:115] Run: docker container inspect minikube --format={{.State.Status}}
I0211 18:10:17.658255  216445 kic.go:420] container "minikube" state is running.
I0211 18:10:17.658506  216445 cli_runner.go:115] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I0211 18:10:17.679991  216445 profile.go:147] Saving config to /home/david/.minikube/profiles/minikube/config.json ...
I0211 18:10:17.680117  216445 machine.go:88] provisioning docker machine ...
I0211 18:10:17.680126  216445 ubuntu.go:169] provisioning hostname "minikube"
I0211 18:10:17.680153  216445 cli_runner.go:115] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0211 18:10:17.701058  216445 main.go:130] libmachine: Using SSH client type: native
I0211 18:10:17.701351  216445 main.go:130] libmachine: &{{{<nil> 0 [] [] []} docker [0x7a0280] 0x7a3360 <nil>  [] 0s} 127.0.0.1 49157 <nil> <nil>}
I0211 18:10:17.701357  216445 main.go:130] libmachine: About to run SSH command:
sudo hostname minikube && echo "minikube" | sudo tee /etc/hostname
I0211 18:10:17.701713  216445 main.go:130] libmachine: Error dialing TCP: ssh: handshake failed: read tcp 127.0.0.1:55890->127.0.0.1:49157: read: connection reset by peer
I0211 18:10:20.858370  216445 main.go:130] libmachine: SSH cmd err, output: <nil>: minikube

I0211 18:10:20.858410  216445 cli_runner.go:115] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0211 18:10:20.881038  216445 main.go:130] libmachine: Using SSH client type: native
I0211 18:10:20.881160  216445 main.go:130] libmachine: &{{{<nil> 0 [] [] []} docker [0x7a0280] 0x7a3360 <nil>  [] 0s} 127.0.0.1 49157 <nil> <nil>}
I0211 18:10:20.881172  216445 main.go:130] libmachine: About to run SSH command:

		if ! grep -xq '.*\sminikube' /etc/hosts; then
			if grep -xq '127.0.1.1\s.*' /etc/hosts; then
				sudo sed -i 's/^127.0.1.1\s.*/127.0.1.1 minikube/g' /etc/hosts;
			else 
				echo '127.0.1.1 minikube' | sudo tee -a /etc/hosts; 
			fi
		fi
I0211 18:10:20.984261  216445 main.go:130] libmachine: SSH cmd err, output: <nil>: 
I0211 18:10:20.984274  216445 ubuntu.go:175] set auth options {CertDir:/home/david/.minikube CaCertPath:/home/david/.minikube/certs/ca.pem CaPrivateKeyPath:/home/david/.minikube/certs/ca-key.pem CaCertRemotePath:/etc/docker/ca.pem ServerCertPath:/home/david/.minikube/machines/server.pem ServerKeyPath:/home/david/.minikube/machines/server-key.pem ClientKeyPath:/home/david/.minikube/certs/key.pem ServerCertRemotePath:/etc/docker/server.pem ServerKeyRemotePath:/etc/docker/server-key.pem ClientCertPath:/home/david/.minikube/certs/cert.pem ServerCertSANs:[] StorePath:/home/david/.minikube}
I0211 18:10:20.984291  216445 ubuntu.go:177] setting up certificates
I0211 18:10:20.984297  216445 provision.go:83] configureAuth start
I0211 18:10:20.984340  216445 cli_runner.go:115] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I0211 18:10:21.009292  216445 provision.go:138] copyHostCerts
I0211 18:10:21.010473  216445 exec_runner.go:144] found /home/david/.minikube/cert.pem, removing ...
I0211 18:10:21.010494  216445 exec_runner.go:207] rm: /home/david/.minikube/cert.pem
I0211 18:10:21.010526  216445 exec_runner.go:151] cp: /home/david/.minikube/certs/cert.pem --> /home/david/.minikube/cert.pem (1119 bytes)
I0211 18:10:21.010810  216445 exec_runner.go:144] found /home/david/.minikube/key.pem, removing ...
I0211 18:10:21.010814  216445 exec_runner.go:207] rm: /home/david/.minikube/key.pem
I0211 18:10:21.010836  216445 exec_runner.go:151] cp: /home/david/.minikube/certs/key.pem --> /home/david/.minikube/key.pem (1679 bytes)
I0211 18:10:21.011106  216445 exec_runner.go:144] found /home/david/.minikube/ca.pem, removing ...
I0211 18:10:21.011109  216445 exec_runner.go:207] rm: /home/david/.minikube/ca.pem
I0211 18:10:21.011132  216445 exec_runner.go:151] cp: /home/david/.minikube/certs/ca.pem --> /home/david/.minikube/ca.pem (1074 bytes)
I0211 18:10:21.011284  216445 provision.go:112] generating server cert: /home/david/.minikube/machines/server.pem ca-key=/home/david/.minikube/certs/ca.pem private-key=/home/david/.minikube/certs/ca-key.pem org=david.minikube san=[192.168.49.2 127.0.0.1 localhost 127.0.0.1 minikube minikube]
I0211 18:10:21.185267  216445 provision.go:172] copyRemoteCerts
I0211 18:10:21.185320  216445 ssh_runner.go:152] Run: sudo mkdir -p /etc/docker /etc/docker /etc/docker
I0211 18:10:21.185357  216445 cli_runner.go:115] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0211 18:10:21.207424  216445 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:49157 SSHKeyPath:/home/david/.minikube/machines/minikube/id_rsa Username:docker}
I0211 18:10:21.296933  216445 ssh_runner.go:319] scp /home/david/.minikube/certs/ca.pem --> /etc/docker/ca.pem (1074 bytes)
I0211 18:10:21.319082  216445 ssh_runner.go:319] scp /home/david/.minikube/machines/server.pem --> /etc/docker/server.pem (1196 bytes)
I0211 18:10:21.332497  216445 ssh_runner.go:319] scp /home/david/.minikube/machines/server-key.pem --> /etc/docker/server-key.pem (1679 bytes)
I0211 18:10:21.345830  216445 provision.go:86] duration metric: configureAuth took 361.527768ms
I0211 18:10:21.345840  216445 ubuntu.go:193] setting minikube options for container-runtime
I0211 18:10:21.345953  216445 config.go:176] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.22.3
I0211 18:10:21.345982  216445 cli_runner.go:115] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0211 18:10:21.371218  216445 main.go:130] libmachine: Using SSH client type: native
I0211 18:10:21.371326  216445 main.go:130] libmachine: &{{{<nil> 0 [] [] []} docker [0x7a0280] 0x7a3360 <nil>  [] 0s} 127.0.0.1 49157 <nil> <nil>}
I0211 18:10:21.371331  216445 main.go:130] libmachine: About to run SSH command:
df --output=fstype / | tail -n 1
I0211 18:10:21.481972  216445 main.go:130] libmachine: SSH cmd err, output: <nil>: overlay

I0211 18:10:21.481996  216445 ubuntu.go:71] root file system type: overlay
I0211 18:10:21.482407  216445 provision.go:309] Updating docker unit: /lib/systemd/system/docker.service ...
I0211 18:10:21.482523  216445 cli_runner.go:115] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0211 18:10:21.510998  216445 main.go:130] libmachine: Using SSH client type: native
I0211 18:10:21.511097  216445 main.go:130] libmachine: &{{{<nil> 0 [] [] []} docker [0x7a0280] 0x7a3360 <nil>  [] 0s} 127.0.0.1 49157 <nil> <nil>}
I0211 18:10:21.511155  216445 main.go:130] libmachine: About to run SSH command:
sudo mkdir -p /lib/systemd/system && printf %!s(MISSING) "[Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
BindsTo=containerd.service
After=network-online.target firewalld.service containerd.service
Wants=network-online.target
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=on-failure



# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP \$MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process

[Install]
WantedBy=multi-user.target
" | sudo tee /lib/systemd/system/docker.service.new
I0211 18:10:21.620705  216445 main.go:130] libmachine: SSH cmd err, output: <nil>: [Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
BindsTo=containerd.service
After=network-online.target firewalld.service containerd.service
Wants=network-online.target
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=on-failure



# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP $MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process

[Install]
WantedBy=multi-user.target

I0211 18:10:21.620756  216445 cli_runner.go:115] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0211 18:10:21.645496  216445 main.go:130] libmachine: Using SSH client type: native
I0211 18:10:21.645596  216445 main.go:130] libmachine: &{{{<nil> 0 [] [] []} docker [0x7a0280] 0x7a3360 <nil>  [] 0s} 127.0.0.1 49157 <nil> <nil>}
I0211 18:10:21.645608  216445 main.go:130] libmachine: About to run SSH command:
sudo diff -u /lib/systemd/system/docker.service /lib/systemd/system/docker.service.new || { sudo mv /lib/systemd/system/docker.service.new /lib/systemd/system/docker.service; sudo systemctl -f daemon-reload && sudo systemctl -f enable docker && sudo systemctl -f restart docker; }
I0211 18:10:21.762962  216445 main.go:130] libmachine: SSH cmd err, output: <nil>: 
I0211 18:10:21.762984  216445 machine.go:91] provisioned docker machine in 4.08285987s
I0211 18:10:21.762998  216445 start.go:267] post-start starting for "minikube" (driver="docker")
I0211 18:10:21.763007  216445 start.go:277] creating required directories: [/etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs]
I0211 18:10:21.763092  216445 ssh_runner.go:152] Run: sudo mkdir -p /etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs
I0211 18:10:21.763155  216445 cli_runner.go:115] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0211 18:10:21.789089  216445 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:49157 SSHKeyPath:/home/david/.minikube/machines/minikube/id_rsa Username:docker}
I0211 18:10:21.867936  216445 ssh_runner.go:152] Run: cat /etc/os-release
I0211 18:10:21.870292  216445 main.go:130] libmachine: Couldn't set key PRIVACY_POLICY_URL, no corresponding struct field found
I0211 18:10:21.870302  216445 main.go:130] libmachine: Couldn't set key VERSION_CODENAME, no corresponding struct field found
I0211 18:10:21.870309  216445 main.go:130] libmachine: Couldn't set key UBUNTU_CODENAME, no corresponding struct field found
I0211 18:10:21.870312  216445 info.go:137] Remote host: Ubuntu 20.04.2 LTS
I0211 18:10:21.870318  216445 filesync.go:126] Scanning /home/david/.minikube/addons for local assets ...
I0211 18:10:21.870518  216445 filesync.go:126] Scanning /home/david/.minikube/files for local assets ...
I0211 18:10:21.870658  216445 start.go:270] post-start completed in 107.653776ms
I0211 18:10:21.870696  216445 ssh_runner.go:152] Run: sh -c "df -h /var | awk 'NR==2{print $5}'"
I0211 18:10:21.870718  216445 cli_runner.go:115] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0211 18:10:21.895203  216445 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:49157 SSHKeyPath:/home/david/.minikube/machines/minikube/id_rsa Username:docker}
I0211 18:10:21.976682  216445 fix.go:57] fixHost completed within 4.778084176s
I0211 18:10:21.976708  216445 start.go:80] releasing machines lock for "minikube", held for 4.778124285s
I0211 18:10:21.976857  216445 cli_runner.go:115] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I0211 18:10:22.017796  216445 ssh_runner.go:152] Run: systemctl --version
I0211 18:10:22.017825  216445 cli_runner.go:115] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0211 18:10:22.017831  216445 ssh_runner.go:152] Run: curl -sS -m 2 https://k8s.gcr.io/
I0211 18:10:22.017859  216445 cli_runner.go:115] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0211 18:10:22.042296  216445 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:49157 SSHKeyPath:/home/david/.minikube/machines/minikube/id_rsa Username:docker}
I0211 18:10:22.042308  216445 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:49157 SSHKeyPath:/home/david/.minikube/machines/minikube/id_rsa Username:docker}
I0211 18:10:22.316190  216445 ssh_runner.go:152] Run: sudo systemctl is-active --quiet service containerd
I0211 18:10:22.343930  216445 ssh_runner.go:152] Run: sudo systemctl cat docker.service
I0211 18:10:22.371562  216445 cruntime.go:255] skipping containerd shutdown because we are bound to it
I0211 18:10:22.371660  216445 ssh_runner.go:152] Run: sudo systemctl is-active --quiet service crio
I0211 18:10:22.401839  216445 ssh_runner.go:152] Run: /bin/bash -c "sudo mkdir -p /etc && printf %!s(MISSING) "runtime-endpoint: unix:///var/run/dockershim.sock
image-endpoint: unix:///var/run/dockershim.sock
" | sudo tee /etc/crictl.yaml"
I0211 18:10:22.439513  216445 ssh_runner.go:152] Run: sudo systemctl unmask docker.service
I0211 18:10:22.529925  216445 ssh_runner.go:152] Run: sudo systemctl enable docker.socket
I0211 18:10:22.596089  216445 ssh_runner.go:152] Run: sudo systemctl cat docker.service
I0211 18:10:22.602770  216445 ssh_runner.go:152] Run: sudo systemctl daemon-reload
I0211 18:10:22.666250  216445 ssh_runner.go:152] Run: sudo systemctl start docker
I0211 18:10:22.673037  216445 ssh_runner.go:152] Run: docker version --format {{.Server.Version}}
I0211 18:10:22.775434  216445 ssh_runner.go:152] Run: docker version --format {{.Server.Version}}
I0211 18:10:22.804218  216445 out.go:203] 🐳  Preparing Kubernetes v1.22.3 on Docker 20.10.8 ...
I0211 18:10:22.804286  216445 cli_runner.go:115] Run: docker network inspect minikube --format "{"Name": "{{.Name}}","Driver": "{{.Driver}}","Subnet": "{{range .IPAM.Config}}{{.Subnet}}{{end}}","Gateway": "{{range .IPAM.Config}}{{.Gateway}}{{end}}","MTU": {{if (index .Options "com.docker.network.driver.mtu")}}{{(index .Options "com.docker.network.driver.mtu")}}{{else}}0{{end}}, "ContainerIPs": [{{range $k,$v := .Containers }}"{{$v.IPv4Address}}",{{end}}]}"
I0211 18:10:22.826606  216445 ssh_runner.go:152] Run: grep 192.168.49.1	host.minikube.internal$ /etc/hosts
I0211 18:10:22.828739  216445 ssh_runner.go:152] Run: /bin/bash -c "{ grep -v $'\thost.minikube.internal$' "/etc/hosts"; echo "192.168.49.1	host.minikube.internal"; } > /tmp/h.$$; sudo cp /tmp/h.$$ "/etc/hosts""
I0211 18:10:22.835033  216445 preload.go:132] Checking if preload exists for k8s version v1.22.3 and runtime docker
I0211 18:10:22.835072  216445 ssh_runner.go:152] Run: docker images --format {{.Repository}}:{{.Tag}}
I0211 18:10:22.860191  216445 docker.go:558] Got preloaded images: -- stdout --
postgres:latest
redis:latest
k8s.gcr.io/kube-apiserver:v1.22.3
k8s.gcr.io/kube-controller-manager:v1.22.3
k8s.gcr.io/kube-scheduler:v1.22.3
k8s.gcr.io/kube-proxy:v1.22.3
kubernetesui/dashboard:v2.3.1
k8s.gcr.io/etcd:3.5.0-0
kubernetesui/metrics-scraper:v1.0.7
k8s.gcr.io/coredns/coredns:v1.8.4
gcr.io/k8s-minikube/storage-provisioner:v5
k8s.gcr.io/pause:3.5
cygnetops/multi-server-pgfix-5-11:latest
stephengrider/multi-worker:latest
stephengrider/multi-client:latest
stephengrider/multi-client:v5

-- /stdout --
I0211 18:10:22.860199  216445 docker.go:489] Images already preloaded, skipping extraction
I0211 18:10:22.860225  216445 ssh_runner.go:152] Run: docker images --format {{.Repository}}:{{.Tag}}
I0211 18:10:22.886345  216445 docker.go:558] Got preloaded images: -- stdout --
postgres:latest
redis:latest
k8s.gcr.io/kube-apiserver:v1.22.3
k8s.gcr.io/kube-controller-manager:v1.22.3
k8s.gcr.io/kube-scheduler:v1.22.3
k8s.gcr.io/kube-proxy:v1.22.3
kubernetesui/dashboard:v2.3.1
k8s.gcr.io/etcd:3.5.0-0
kubernetesui/metrics-scraper:v1.0.7
k8s.gcr.io/coredns/coredns:v1.8.4
gcr.io/k8s-minikube/storage-provisioner:v5
k8s.gcr.io/pause:3.5
cygnetops/multi-server-pgfix-5-11:latest
stephengrider/multi-worker:latest
stephengrider/multi-client:latest
stephengrider/multi-client:v5

-- /stdout --
I0211 18:10:22.886356  216445 cache_images.go:79] Images are preloaded, skipping loading
I0211 18:10:22.886384  216445 ssh_runner.go:152] Run: docker info --format {{.CgroupDriver}}
I0211 18:10:23.093619  216445 cni.go:93] Creating CNI manager for ""
I0211 18:10:23.093625  216445 cni.go:167] CNI unnecessary in this configuration, recommending no CNI
I0211 18:10:23.093631  216445 kubeadm.go:87] Using pod CIDR: 10.244.0.0/16
I0211 18:10:23.093640  216445 kubeadm.go:153] kubeadm options: {CertDir:/var/lib/minikube/certs ServiceCIDR:10.96.0.0/12 PodSubnet:10.244.0.0/16 AdvertiseAddress:192.168.49.2 APIServerPort:8443 KubernetesVersion:v1.22.3 EtcdDataDir:/var/lib/minikube/etcd EtcdExtraArgs:map[] ClusterName:minikube NodeName:minikube DNSDomain:cluster.local CRISocket:/var/run/dockershim.sock ImageRepository: ComponentOptions:[{Component:apiServer ExtraArgs:map[enable-admission-plugins:NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota] Pairs:map[certSANs:["127.0.0.1", "localhost", "192.168.49.2"]]} {Component:controllerManager ExtraArgs:map[allocate-node-cidrs:true leader-elect:false] Pairs:map[]} {Component:scheduler ExtraArgs:map[leader-elect:false] Pairs:map[]}] FeatureArgs:map[] NoTaintMaster:true NodeIP:192.168.49.2 CgroupDriver:cgroupfs ClientCAFile:/var/lib/minikube/certs/ca.crt StaticPodPath:/etc/kubernetes/manifests ControlPlaneAddress:control-plane.minikube.internal KubeProxyOptions:map[]}
I0211 18:10:23.093714  216445 kubeadm.go:157] kubeadm config:
apiVersion: kubeadm.k8s.io/v1beta2
kind: InitConfiguration
localAPIEndpoint:
  advertiseAddress: 192.168.49.2
  bindPort: 8443
bootstrapTokens:
  - groups:
      - system:bootstrappers:kubeadm:default-node-token
    ttl: 24h0m0s
    usages:
      - signing
      - authentication
nodeRegistration:
  criSocket: /var/run/dockershim.sock
  name: "minikube"
  kubeletExtraArgs:
    node-ip: 192.168.49.2
  taints: []
---
apiVersion: kubeadm.k8s.io/v1beta2
kind: ClusterConfiguration
apiServer:
  certSANs: ["127.0.0.1", "localhost", "192.168.49.2"]
  extraArgs:
    enable-admission-plugins: "NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota"
controllerManager:
  extraArgs:
    allocate-node-cidrs: "true"
    leader-elect: "false"
scheduler:
  extraArgs:
    leader-elect: "false"
certificatesDir: /var/lib/minikube/certs
clusterName: mk
controlPlaneEndpoint: control-plane.minikube.internal:8443
dns:
  type: CoreDNS
etcd:
  local:
    dataDir: /var/lib/minikube/etcd
    extraArgs:
      proxy-refresh-interval: "70000"
kubernetesVersion: v1.22.3
networking:
  dnsDomain: cluster.local
  podSubnet: "10.244.0.0/16"
  serviceSubnet: 10.96.0.0/12
---
apiVersion: kubelet.config.k8s.io/v1beta1
kind: KubeletConfiguration
authentication:
  x509:
    clientCAFile: /var/lib/minikube/certs/ca.crt
cgroupDriver: cgroupfs
clusterDomain: "cluster.local"
# disable disk resource management by default
imageGCHighThresholdPercent: 100
evictionHard:
  nodefs.available: "0%!"(MISSING)
  nodefs.inodesFree: "0%!"(MISSING)
  imagefs.available: "0%!"(MISSING)
failSwapOn: false
staticPodPath: /etc/kubernetes/manifests
---
apiVersion: kubeproxy.config.k8s.io/v1alpha1
kind: KubeProxyConfiguration
clusterCIDR: "10.244.0.0/16"
metricsBindAddress: 0.0.0.0:10249
conntrack:
  maxPerCore: 0
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_established"
  tcpEstablishedTimeout: 0s
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_close"
  tcpCloseWaitTimeout: 0s

I0211 18:10:23.093771  216445 kubeadm.go:909] kubelet [Unit]
Wants=docker.socket

[Service]
ExecStart=
ExecStart=/var/lib/minikube/binaries/v1.22.3/kubelet --bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --config=/var/lib/kubelet/config.yaml --container-runtime=docker --hostname-override=minikube --kubeconfig=/etc/kubernetes/kubelet.conf --node-ip=192.168.49.2

[Install]
 config:
{KubernetesVersion:v1.22.3 ClusterName:minikube Namespace:default APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin: FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI: NodeIP: NodePort:8443 NodeName:}
I0211 18:10:23.093801  216445 ssh_runner.go:152] Run: sudo ls /var/lib/minikube/binaries/v1.22.3
I0211 18:10:23.099554  216445 binaries.go:44] Found k8s binaries, skipping transfer
I0211 18:10:23.099586  216445 ssh_runner.go:152] Run: sudo mkdir -p /etc/systemd/system/kubelet.service.d /lib/systemd/system /var/tmp/minikube
I0211 18:10:23.104766  216445 ssh_runner.go:319] scp memory --> /etc/systemd/system/kubelet.service.d/10-kubeadm.conf (334 bytes)
I0211 18:10:23.113176  216445 ssh_runner.go:319] scp memory --> /lib/systemd/system/kubelet.service (352 bytes)
I0211 18:10:23.122817  216445 ssh_runner.go:319] scp memory --> /var/tmp/minikube/kubeadm.yaml.new (2051 bytes)
I0211 18:10:23.131886  216445 ssh_runner.go:152] Run: grep 192.168.49.2	control-plane.minikube.internal$ /etc/hosts
I0211 18:10:23.133827  216445 ssh_runner.go:152] Run: /bin/bash -c "{ grep -v $'\tcontrol-plane.minikube.internal$' "/etc/hosts"; echo "192.168.49.2	control-plane.minikube.internal"; } > /tmp/h.$$; sudo cp /tmp/h.$$ "/etc/hosts""
I0211 18:10:23.140266  216445 certs.go:54] Setting up /home/david/.minikube/profiles/minikube for IP: 192.168.49.2
I0211 18:10:23.140496  216445 certs.go:182] skipping minikubeCA CA generation: /home/david/.minikube/ca.key
I0211 18:10:23.140641  216445 certs.go:182] skipping proxyClientCA CA generation: /home/david/.minikube/proxy-client-ca.key
I0211 18:10:23.140799  216445 certs.go:298] skipping minikube-user signed cert generation: /home/david/.minikube/profiles/minikube/client.key
I0211 18:10:23.140947  216445 certs.go:298] skipping minikube signed cert generation: /home/david/.minikube/profiles/minikube/apiserver.key.dd3b5fb2
I0211 18:10:23.141094  216445 certs.go:298] skipping aggregator signed cert generation: /home/david/.minikube/profiles/minikube/proxy-client.key
I0211 18:10:23.141156  216445 certs.go:388] found cert: /home/david/.minikube/certs/home/david/.minikube/certs/ca-key.pem (1675 bytes)
I0211 18:10:23.141172  216445 certs.go:388] found cert: /home/david/.minikube/certs/home/david/.minikube/certs/ca.pem (1074 bytes)
I0211 18:10:23.141186  216445 certs.go:388] found cert: /home/david/.minikube/certs/home/david/.minikube/certs/cert.pem (1119 bytes)
I0211 18:10:23.141198  216445 certs.go:388] found cert: /home/david/.minikube/certs/home/david/.minikube/certs/key.pem (1679 bytes)
I0211 18:10:23.142065  216445 ssh_runner.go:319] scp /home/david/.minikube/profiles/minikube/apiserver.crt --> /var/lib/minikube/certs/apiserver.crt (1399 bytes)
I0211 18:10:23.157128  216445 ssh_runner.go:319] scp /home/david/.minikube/profiles/minikube/apiserver.key --> /var/lib/minikube/certs/apiserver.key (1675 bytes)
I0211 18:10:23.169231  216445 ssh_runner.go:319] scp /home/david/.minikube/profiles/minikube/proxy-client.crt --> /var/lib/minikube/certs/proxy-client.crt (1147 bytes)
I0211 18:10:23.182302  216445 ssh_runner.go:319] scp /home/david/.minikube/profiles/minikube/proxy-client.key --> /var/lib/minikube/certs/proxy-client.key (1679 bytes)
I0211 18:10:23.195397  216445 ssh_runner.go:319] scp /home/david/.minikube/ca.crt --> /var/lib/minikube/certs/ca.crt (1111 bytes)
I0211 18:10:23.207970  216445 ssh_runner.go:319] scp /home/david/.minikube/ca.key --> /var/lib/minikube/certs/ca.key (1675 bytes)
I0211 18:10:23.220630  216445 ssh_runner.go:319] scp /home/david/.minikube/proxy-client-ca.crt --> /var/lib/minikube/certs/proxy-client-ca.crt (1119 bytes)
I0211 18:10:23.233102  216445 ssh_runner.go:319] scp /home/david/.minikube/proxy-client-ca.key --> /var/lib/minikube/certs/proxy-client-ca.key (1679 bytes)
I0211 18:10:23.246095  216445 ssh_runner.go:319] scp /home/david/.minikube/ca.crt --> /usr/share/ca-certificates/minikubeCA.pem (1111 bytes)
I0211 18:10:23.259197  216445 ssh_runner.go:319] scp memory --> /var/lib/minikube/kubeconfig (738 bytes)
I0211 18:10:23.268135  216445 ssh_runner.go:152] Run: openssl version
I0211 18:10:23.274136  216445 ssh_runner.go:152] Run: sudo /bin/bash -c "test -s /usr/share/ca-certificates/minikubeCA.pem && ln -fs /usr/share/ca-certificates/minikubeCA.pem /etc/ssl/certs/minikubeCA.pem"
I0211 18:10:23.280295  216445 ssh_runner.go:152] Run: ls -la /usr/share/ca-certificates/minikubeCA.pem
I0211 18:10:23.282270  216445 certs.go:431] hashing: -rw-r--r-- 1 root root 1111 Jan 16 12:53 /usr/share/ca-certificates/minikubeCA.pem
I0211 18:10:23.282295  216445 ssh_runner.go:152] Run: openssl x509 -hash -noout -in /usr/share/ca-certificates/minikubeCA.pem
I0211 18:10:23.285922  216445 ssh_runner.go:152] Run: sudo /bin/bash -c "test -L /etc/ssl/certs/b5213941.0 || ln -fs /etc/ssl/certs/minikubeCA.pem /etc/ssl/certs/b5213941.0"
I0211 18:10:23.290598  216445 kubeadm.go:390] StartCluster: {Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.28@sha256:4780f1897569d2bf77aafb3d133a08d42b4fe61127f06fcfc90c2c5d902d893c Memory:3900 CPUs:2 DiskSize:20000 VMDriver: Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.22.3 ClusterName:minikube Namespace:default APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin: FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI: NodeIP: NodePort:8443 NodeName:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.22.3 ControlPlane:true Worker:true}] Addons:map[ambassador:false auto-pause:false csi-hostpath-driver:false dashboard:false default-storageclass:true efk:false freshpod:false gcp-auth:false gvisor:false helm-tiller:false ingress:false ingress-dns:false istio:false istio-provisioner:false kubevirt:false logviewer:false metallb:false metrics-server:false nvidia-driver-installer:false nvidia-gpu-device-plugin:false olm:false pod-security-policy:false portainer:false registry:false registry-aliases:false registry-creds:false storage-provisioner:true storage-provisioner-gluster:false volumesnapshots:false] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/home/david:/minikube-host}
I0211 18:10:23.290674  216445 ssh_runner.go:152] Run: docker ps --filter status=paused --filter=name=k8s_.*_(kube-system)_ --format={{.ID}}
I0211 18:10:23.313204  216445 ssh_runner.go:152] Run: sudo ls /var/lib/kubelet/kubeadm-flags.env /var/lib/kubelet/config.yaml /var/lib/minikube/etcd
I0211 18:10:23.318475  216445 kubeadm.go:401] found existing configuration files, will attempt cluster restart
I0211 18:10:23.318482  216445 kubeadm.go:600] restartCluster start
I0211 18:10:23.318512  216445 ssh_runner.go:152] Run: sudo test -d /data/minikube
I0211 18:10:23.323606  216445 kubeadm.go:126] /data/minikube skipping compat symlinks: sudo test -d /data/minikube: Process exited with status 1
stdout:

stderr:
I0211 18:10:23.324028  216445 kubeconfig.go:116] verify returned: extract IP: "minikube" does not appear in /home/david/.kube/config
I0211 18:10:23.324084  216445 kubeconfig.go:127] "minikube" context is missing from /home/david/.kube/config - will repair!
I0211 18:10:23.324251  216445 lock.go:35] WriteFile acquiring /home/david/.kube/config: {Name:mkb997a042f6708b09a6e22f8c8bb7a2e4bd5f08 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0211 18:10:23.329024  216445 ssh_runner.go:152] Run: sudo diff -u /var/tmp/minikube/kubeadm.yaml /var/tmp/minikube/kubeadm.yaml.new
I0211 18:10:23.333898  216445 api_server.go:165] Checking apiserver status ...
I0211 18:10:23.333932  216445 ssh_runner.go:152] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0211 18:10:23.343778  216445 api_server.go:169] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0211 18:10:23.544868  216445 api_server.go:165] Checking apiserver status ...
I0211 18:10:23.544978  216445 ssh_runner.go:152] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0211 18:10:23.583666  216445 api_server.go:169] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0211 18:10:23.744881  216445 api_server.go:165] Checking apiserver status ...
I0211 18:10:23.744977  216445 ssh_runner.go:152] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0211 18:10:23.760167  216445 api_server.go:169] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0211 18:10:23.944037  216445 api_server.go:165] Checking apiserver status ...
I0211 18:10:23.944125  216445 ssh_runner.go:152] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0211 18:10:23.957188  216445 api_server.go:169] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0211 18:10:24.144258  216445 api_server.go:165] Checking apiserver status ...
I0211 18:10:24.144361  216445 ssh_runner.go:152] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0211 18:10:24.181511  216445 api_server.go:169] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0211 18:10:24.344434  216445 api_server.go:165] Checking apiserver status ...
I0211 18:10:24.344529  216445 ssh_runner.go:152] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0211 18:10:24.385774  216445 api_server.go:169] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0211 18:10:24.544680  216445 api_server.go:165] Checking apiserver status ...
I0211 18:10:24.544792  216445 ssh_runner.go:152] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0211 18:10:24.578967  216445 api_server.go:169] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0211 18:10:24.744090  216445 api_server.go:165] Checking apiserver status ...
I0211 18:10:24.744183  216445 ssh_runner.go:152] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0211 18:10:24.770317  216445 api_server.go:169] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0211 18:10:24.944188  216445 api_server.go:165] Checking apiserver status ...
I0211 18:10:24.944278  216445 ssh_runner.go:152] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0211 18:10:24.964633  216445 api_server.go:169] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0211 18:10:25.144763  216445 api_server.go:165] Checking apiserver status ...
I0211 18:10:25.144874  216445 ssh_runner.go:152] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0211 18:10:25.182798  216445 api_server.go:169] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0211 18:10:25.344634  216445 api_server.go:165] Checking apiserver status ...
I0211 18:10:25.344734  216445 ssh_runner.go:152] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0211 18:10:25.362699  216445 api_server.go:169] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0211 18:10:25.543901  216445 api_server.go:165] Checking apiserver status ...
I0211 18:10:25.544026  216445 ssh_runner.go:152] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0211 18:10:25.581040  216445 api_server.go:169] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0211 18:10:25.744536  216445 api_server.go:165] Checking apiserver status ...
I0211 18:10:25.744612  216445 ssh_runner.go:152] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0211 18:10:25.770097  216445 api_server.go:169] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0211 18:10:25.943923  216445 api_server.go:165] Checking apiserver status ...
I0211 18:10:25.944060  216445 ssh_runner.go:152] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0211 18:10:25.958058  216445 api_server.go:169] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0211 18:10:26.144085  216445 api_server.go:165] Checking apiserver status ...
I0211 18:10:26.144181  216445 ssh_runner.go:152] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0211 18:10:26.158466  216445 api_server.go:169] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0211 18:10:26.344644  216445 api_server.go:165] Checking apiserver status ...
I0211 18:10:26.344759  216445 ssh_runner.go:152] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0211 18:10:26.357762  216445 api_server.go:169] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0211 18:10:26.357769  216445 api_server.go:165] Checking apiserver status ...
I0211 18:10:26.357794  216445 ssh_runner.go:152] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0211 18:10:26.367805  216445 api_server.go:169] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0211 18:10:26.367813  216445 kubeadm.go:575] needs reconfigure: apiserver error: timed out waiting for the condition
I0211 18:10:26.367816  216445 kubeadm.go:1032] stopping kube-system containers ...
I0211 18:10:26.367847  216445 ssh_runner.go:152] Run: docker ps -a --filter=name=k8s_.*_(kube-system)_ --format={{.ID}}
I0211 18:10:26.397429  216445 docker.go:390] Stopping containers: [821a6243ac9c 8fa8a2d96f97 8d921ccbda79 d0c3eecab787 35f61d1bf74b 0efc5afdfa11 3229ea58fd90 93d9313e954e 63b4e95072d7 bba2afb05bca 0169b6e51edf 9dac9d6cd777 0b2c83b5a88d b33a782ac219 4675965c509d 9c76a529e0ba 1798edeb7fe0 48c26d95d276 cfa91a0a7d0e bc22352c63bd dfd73f05203b c4d33f095ba5 78da077b3ab5 664e53e4f961 c072fa4f34a6 730dd527d8fe 03e7af9eb869 0dd30d6749ae]
I0211 18:10:26.397471  216445 ssh_runner.go:152] Run: docker stop 821a6243ac9c 8fa8a2d96f97 8d921ccbda79 d0c3eecab787 35f61d1bf74b 0efc5afdfa11 3229ea58fd90 93d9313e954e 63b4e95072d7 bba2afb05bca 0169b6e51edf 9dac9d6cd777 0b2c83b5a88d b33a782ac219 4675965c509d 9c76a529e0ba 1798edeb7fe0 48c26d95d276 cfa91a0a7d0e bc22352c63bd dfd73f05203b c4d33f095ba5 78da077b3ab5 664e53e4f961 c072fa4f34a6 730dd527d8fe 03e7af9eb869 0dd30d6749ae
I0211 18:10:26.422811  216445 ssh_runner.go:152] Run: sudo systemctl stop kubelet
I0211 18:10:26.430143  216445 ssh_runner.go:152] Run: sudo ls -la /etc/kubernetes/admin.conf /etc/kubernetes/kubelet.conf /etc/kubernetes/controller-manager.conf /etc/kubernetes/scheduler.conf
I0211 18:10:26.434809  216445 kubeadm.go:154] found existing configuration files:
-rw------- 1 root root 5643 Jan 16 12:53 /etc/kubernetes/admin.conf
-rw------- 1 root root 5656 Feb  9 18:20 /etc/kubernetes/controller-manager.conf
-rw------- 1 root root 1971 Jan 16 12:53 /etc/kubernetes/kubelet.conf
-rw------- 1 root root 5604 Feb  9 18:20 /etc/kubernetes/scheduler.conf

I0211 18:10:26.434837  216445 ssh_runner.go:152] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/admin.conf
I0211 18:10:26.439901  216445 ssh_runner.go:152] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/kubelet.conf
I0211 18:10:26.445193  216445 ssh_runner.go:152] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/controller-manager.conf
I0211 18:10:26.450093  216445 kubeadm.go:165] "https://control-plane.minikube.internal:8443" may not be in /etc/kubernetes/controller-manager.conf - will remove: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/controller-manager.conf: Process exited with status 1
stdout:

stderr:
I0211 18:10:26.450115  216445 ssh_runner.go:152] Run: sudo rm -f /etc/kubernetes/controller-manager.conf
I0211 18:10:26.455163  216445 ssh_runner.go:152] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/scheduler.conf
I0211 18:10:26.460006  216445 kubeadm.go:165] "https://control-plane.minikube.internal:8443" may not be in /etc/kubernetes/scheduler.conf - will remove: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/scheduler.conf: Process exited with status 1
stdout:

stderr:
I0211 18:10:26.460031  216445 ssh_runner.go:152] Run: sudo rm -f /etc/kubernetes/scheduler.conf
I0211 18:10:26.464110  216445 ssh_runner.go:152] Run: sudo cp /var/tmp/minikube/kubeadm.yaml.new /var/tmp/minikube/kubeadm.yaml
I0211 18:10:26.468445  216445 kubeadm.go:676] reconfiguring cluster from /var/tmp/minikube/kubeadm.yaml
I0211 18:10:26.468452  216445 ssh_runner.go:152] Run: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.22.3:$PATH" kubeadm init phase certs all --config /var/tmp/minikube/kubeadm.yaml"
I0211 18:10:26.583052  216445 ssh_runner.go:152] Run: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.22.3:$PATH" kubeadm init phase kubeconfig all --config /var/tmp/minikube/kubeadm.yaml"
I0211 18:10:27.152591  216445 ssh_runner.go:152] Run: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.22.3:$PATH" kubeadm init phase kubelet-start --config /var/tmp/minikube/kubeadm.yaml"
I0211 18:10:27.260943  216445 ssh_runner.go:152] Run: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.22.3:$PATH" kubeadm init phase control-plane all --config /var/tmp/minikube/kubeadm.yaml"
I0211 18:10:27.299294  216445 ssh_runner.go:152] Run: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.22.3:$PATH" kubeadm init phase etcd local --config /var/tmp/minikube/kubeadm.yaml"
I0211 18:10:27.334101  216445 api_server.go:51] waiting for apiserver process to appear ...
I0211 18:10:27.334131  216445 ssh_runner.go:152] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0211 18:10:27.843971  216445 ssh_runner.go:152] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0211 18:10:28.344832  216445 ssh_runner.go:152] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0211 18:10:28.844439  216445 ssh_runner.go:152] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0211 18:10:28.860511  216445 api_server.go:71] duration metric: took 1.526407751s to wait for apiserver process to appear ...
I0211 18:10:28.860522  216445 api_server.go:87] waiting for apiserver healthz status ...
I0211 18:10:28.860527  216445 api_server.go:240] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I0211 18:10:28.860822  216445 api_server.go:256] stopped: https://192.168.49.2:8443/healthz: Get "https://192.168.49.2:8443/healthz": dial tcp 192.168.49.2:8443: connect: connection refused
I0211 18:10:29.361176  216445 api_server.go:240] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I0211 18:10:29.361496  216445 api_server.go:256] stopped: https://192.168.49.2:8443/healthz: Get "https://192.168.49.2:8443/healthz": dial tcp 192.168.49.2:8443: connect: connection refused
I0211 18:10:29.861599  216445 api_server.go:240] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I0211 18:10:34.466775  216445 api_server.go:266] https://192.168.49.2:8443/healthz returned 403:
{"kind":"Status","apiVersion":"v1","metadata":{},"status":"Failure","message":"forbidden: User \"system:anonymous\" cannot get path \"/healthz\"","reason":"Forbidden","details":{},"code":403}
W0211 18:10:34.466789  216445 api_server.go:102] status: https://192.168.49.2:8443/healthz returned error 403:
{"kind":"Status","apiVersion":"v1","metadata":{},"status":"Failure","message":"forbidden: User \"system:anonymous\" cannot get path \"/healthz\"","reason":"Forbidden","details":{},"code":403}
I0211 18:10:34.861478  216445 api_server.go:240] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I0211 18:10:34.959070  216445 api_server.go:266] https://192.168.49.2:8443/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/bootstrap-controller ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[-]poststarthook/priority-and-fairness-config-producer failed: reason withheld
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
healthz check failed
W0211 18:10:34.959108  216445 api_server.go:102] status: https://192.168.49.2:8443/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/bootstrap-controller ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[-]poststarthook/priority-and-fairness-config-producer failed: reason withheld
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
healthz check failed
I0211 18:10:35.361143  216445 api_server.go:240] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I0211 18:10:35.364255  216445 api_server.go:266] https://192.168.49.2:8443/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/bootstrap-controller ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
healthz check failed
W0211 18:10:35.364263  216445 api_server.go:102] status: https://192.168.49.2:8443/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/bootstrap-controller ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
healthz check failed
I0211 18:10:35.861364  216445 api_server.go:240] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I0211 18:10:35.864845  216445 api_server.go:266] https://192.168.49.2:8443/healthz returned 200:
ok
I0211 18:10:35.870274  216445 api_server.go:140] control plane version: v1.22.3
I0211 18:10:35.870280  216445 api_server.go:130] duration metric: took 7.009755578s to wait for apiserver health ...
I0211 18:10:35.870284  216445 cni.go:93] Creating CNI manager for ""
I0211 18:10:35.870287  216445 cni.go:167] CNI unnecessary in this configuration, recommending no CNI
I0211 18:10:35.870290  216445 system_pods.go:43] waiting for kube-system pods to appear ...
I0211 18:10:35.878319  216445 system_pods.go:59] 7 kube-system pods found
I0211 18:10:35.878332  216445 system_pods.go:61] "coredns-78fcd69978-zszqt" [604119a5-9994-49d7-b00f-e5bb84d293cc] Running
I0211 18:10:35.878342  216445 system_pods.go:61] "etcd-minikube" [2b2c6fa7-ea92-40fb-bcec-ca051b05713c] Running / Ready:ContainersNotReady (containers with unready status: [etcd]) / ContainersReady:ContainersNotReady (containers with unready status: [etcd])
I0211 18:10:35.878349  216445 system_pods.go:61] "kube-apiserver-minikube" [b9a051e1-f74b-43af-8414-189adc1fca52] Running / Ready:ContainersNotReady (containers with unready status: [kube-apiserver]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-apiserver])
I0211 18:10:35.878354  216445 system_pods.go:61] "kube-controller-manager-minikube" [650144f5-8813-4ae2-adeb-25548e009b36] Running
I0211 18:10:35.878360  216445 system_pods.go:61] "kube-proxy-cp82l" [6337ebd8-8d1d-45c1-bc9f-9be00f980fba] Running
I0211 18:10:35.878364  216445 system_pods.go:61] "kube-scheduler-minikube" [213ef396-43c4-452d-a372-0395d2d0bd30] Running
I0211 18:10:35.878367  216445 system_pods.go:61] "storage-provisioner" [c4beb811-f0fd-4d07-b50a-dc668a89bf50] Running
I0211 18:10:35.878371  216445 system_pods.go:74] duration metric: took 8.077741ms to wait for pod list to return data ...
I0211 18:10:35.878376  216445 node_conditions.go:102] verifying NodePressure condition ...
I0211 18:10:35.880425  216445 node_conditions.go:122] node storage ephemeral capacity is 488949800Ki
I0211 18:10:35.880434  216445 node_conditions.go:123] node cpu capacity is 12
I0211 18:10:35.880440  216445 node_conditions.go:105] duration metric: took 2.061535ms to run NodePressure ...
I0211 18:10:35.880456  216445 ssh_runner.go:152] Run: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.22.3:$PATH" kubeadm init phase addon all --config /var/tmp/minikube/kubeadm.yaml"
I0211 18:10:35.992295  216445 ssh_runner.go:152] Run: /bin/bash -c "cat /proc/$(pgrep kube-apiserver)/oom_adj"
I0211 18:10:36.001688  216445 ops.go:34] apiserver oom_adj: -16
I0211 18:10:36.001695  216445 kubeadm.go:604] restartCluster took 12.683210662s
I0211 18:10:36.001699  216445 kubeadm.go:392] StartCluster complete in 12.711106586s
I0211 18:10:36.001707  216445 settings.go:142] acquiring lock: {Name:mk8e5ff823440a033592b7064c4117c94c042f73 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0211 18:10:36.001788  216445 settings.go:150] Updating kubeconfig:  /home/david/.kube/config
I0211 18:10:36.002168  216445 lock.go:35] WriteFile acquiring /home/david/.kube/config: {Name:mkb997a042f6708b09a6e22f8c8bb7a2e4bd5f08 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0211 18:10:36.004801  216445 kapi.go:244] deployment "coredns" in namespace "kube-system" and context "minikube" rescaled to 1
I0211 18:10:36.004828  216445 start.go:229] Will wait 6m0s for node &{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.22.3 ControlPlane:true Worker:true}
I0211 18:10:36.004847  216445 ssh_runner.go:152] Run: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.22.3/kubectl --kubeconfig=/var/lib/minikube/kubeconfig -n kube-system get configmap coredns -o yaml"
I0211 18:10:36.006442  216445 out.go:176] 🔎  Verifying Kubernetes components...
I0211 18:10:36.006488  216445 ssh_runner.go:152] Run: sudo systemctl is-active --quiet service kubelet
I0211 18:10:36.004891  216445 addons.go:415] enableAddons start: toEnable=map[ambassador:false auto-pause:false csi-hostpath-driver:false dashboard:false default-storageclass:true efk:false freshpod:false gcp-auth:false gvisor:false helm-tiller:false ingress:false ingress-dns:false istio:false istio-provisioner:false kubevirt:false logviewer:false metallb:false metrics-server:false nvidia-driver-installer:false nvidia-gpu-device-plugin:false olm:false pod-security-policy:false portainer:false registry:false registry-aliases:false registry-creds:false storage-provisioner:true storage-provisioner-gluster:false volumesnapshots:false], additional=[]
I0211 18:10:36.004977  216445 config.go:176] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.22.3
I0211 18:10:36.006525  216445 addons.go:65] Setting storage-provisioner=true in profile "minikube"
I0211 18:10:36.006527  216445 addons.go:65] Setting default-storageclass=true in profile "minikube"
I0211 18:10:36.006537  216445 addons.go:153] Setting addon storage-provisioner=true in "minikube"
I0211 18:10:36.006538  216445 addons_storage_classes.go:33] enableOrDisableStorageClasses default-storageclass=true on "minikube"
W0211 18:10:36.006542  216445 addons.go:165] addon storage-provisioner should already be in state true
I0211 18:10:36.006557  216445 host.go:66] Checking if "minikube" exists ...
I0211 18:10:36.006890  216445 cli_runner.go:115] Run: docker container inspect minikube --format={{.State.Status}}
I0211 18:10:36.006995  216445 cli_runner.go:115] Run: docker container inspect minikube --format={{.State.Status}}
I0211 18:10:36.032808  216445 out.go:176]     ▪ Using image gcr.io/k8s-minikube/storage-provisioner:v5
I0211 18:10:36.032873  216445 addons.go:348] installing /etc/kubernetes/addons/storage-provisioner.yaml
I0211 18:10:36.032878  216445 ssh_runner.go:319] scp memory --> /etc/kubernetes/addons/storage-provisioner.yaml (2676 bytes)
I0211 18:10:36.032905  216445 cli_runner.go:115] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0211 18:10:36.035463  216445 addons.go:153] Setting addon default-storageclass=true in "minikube"
W0211 18:10:36.035471  216445 addons.go:165] addon default-storageclass should already be in state true
I0211 18:10:36.035488  216445 host.go:66] Checking if "minikube" exists ...
I0211 18:10:36.035742  216445 cli_runner.go:115] Run: docker container inspect minikube --format={{.State.Status}}
I0211 18:10:36.056573  216445 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:49157 SSHKeyPath:/home/david/.minikube/machines/minikube/id_rsa Username:docker}
I0211 18:10:36.060236  216445 addons.go:348] installing /etc/kubernetes/addons/storageclass.yaml
I0211 18:10:36.060244  216445 ssh_runner.go:319] scp memory --> /etc/kubernetes/addons/storageclass.yaml (271 bytes)
I0211 18:10:36.060276  216445 cli_runner.go:115] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0211 18:10:36.082567  216445 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:49157 SSHKeyPath:/home/david/.minikube/machines/minikube/id_rsa Username:docker}
I0211 18:10:36.139904  216445 ssh_runner.go:152] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.22.3/kubectl apply -f /etc/kubernetes/addons/storage-provisioner.yaml
I0211 18:10:36.162528  216445 ssh_runner.go:152] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.22.3/kubectl apply -f /etc/kubernetes/addons/storageclass.yaml
I0211 18:10:36.318053  216445 start.go:719] CoreDNS already contains "host.minikube.internal" host record, skipping...
I0211 18:10:36.318070  216445 api_server.go:51] waiting for apiserver process to appear ...
I0211 18:10:36.318097  216445 ssh_runner.go:152] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0211 18:10:36.378431  216445 api_server.go:71] duration metric: took 373.582054ms to wait for apiserver process to appear ...
I0211 18:10:36.378447  216445 api_server.go:87] waiting for apiserver healthz status ...
I0211 18:10:36.378452  216445 api_server.go:240] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I0211 18:10:36.380398  216445 out.go:176] 🌟  Enabled addons: storage-provisioner, default-storageclass
I0211 18:10:36.380413  216445 addons.go:417] enableAddons completed in 375.526794ms
I0211 18:10:36.382178  216445 api_server.go:266] https://192.168.49.2:8443/healthz returned 200:
ok
I0211 18:10:36.382623  216445 api_server.go:140] control plane version: v1.22.3
I0211 18:10:36.382628  216445 api_server.go:130] duration metric: took 4.177929ms to wait for apiserver health ...
I0211 18:10:36.382631  216445 system_pods.go:43] waiting for kube-system pods to appear ...
I0211 18:10:36.385430  216445 system_pods.go:59] 7 kube-system pods found
I0211 18:10:36.385438  216445 system_pods.go:61] "coredns-78fcd69978-zszqt" [604119a5-9994-49d7-b00f-e5bb84d293cc] Running
I0211 18:10:36.385443  216445 system_pods.go:61] "etcd-minikube" [2b2c6fa7-ea92-40fb-bcec-ca051b05713c] Running / Ready:ContainersNotReady (containers with unready status: [etcd]) / ContainersReady:ContainersNotReady (containers with unready status: [etcd])
I0211 18:10:36.385447  216445 system_pods.go:61] "kube-apiserver-minikube" [b9a051e1-f74b-43af-8414-189adc1fca52] Running / Ready:ContainersNotReady (containers with unready status: [kube-apiserver]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-apiserver])
I0211 18:10:36.385451  216445 system_pods.go:61] "kube-controller-manager-minikube" [650144f5-8813-4ae2-adeb-25548e009b36] Running
I0211 18:10:36.385454  216445 system_pods.go:61] "kube-proxy-cp82l" [6337ebd8-8d1d-45c1-bc9f-9be00f980fba] Running
I0211 18:10:36.385456  216445 system_pods.go:61] "kube-scheduler-minikube" [213ef396-43c4-452d-a372-0395d2d0bd30] Running
I0211 18:10:36.385459  216445 system_pods.go:61] "storage-provisioner" [c4beb811-f0fd-4d07-b50a-dc668a89bf50] Running / Ready:ContainersNotReady (containers with unready status: [storage-provisioner]) / ContainersReady:ContainersNotReady (containers with unready status: [storage-provisioner])
I0211 18:10:36.385461  216445 system_pods.go:74] duration metric: took 2.827989ms to wait for pod list to return data ...
I0211 18:10:36.385465  216445 kubeadm.go:547] duration metric: took 380.621657ms to wait for : map[apiserver:true system_pods:true] ...
I0211 18:10:36.385473  216445 node_conditions.go:102] verifying NodePressure condition ...
I0211 18:10:36.386667  216445 node_conditions.go:122] node storage ephemeral capacity is 488949800Ki
I0211 18:10:36.386675  216445 node_conditions.go:123] node cpu capacity is 12
I0211 18:10:36.386681  216445 node_conditions.go:105] duration metric: took 1.198349ms to run NodePressure ...
I0211 18:10:36.386687  216445 start.go:234] waiting for startup goroutines ...
I0211 18:10:36.494231  216445 start.go:473] kubectl: 1.23.1, cluster: 1.22.3 (minor skew: 1)
I0211 18:10:36.495775  216445 out.go:176] 🏄  Done! kubectl is now configured to use "minikube" cluster and "default" namespace by default

* 
* ==> Docker <==
* -- Logs begin at Fri 2022-02-11 18:10:17 UTC, end at Fri 2022-02-11 18:33:48 UTC. --
Feb 11 18:10:18 minikube systemd[1]: Starting Docker Application Container Engine...
Feb 11 18:10:18 minikube dockerd[211]: time="2022-02-11T18:10:18.221283460Z" level=info msg="Starting up"
Feb 11 18:10:18 minikube dockerd[211]: time="2022-02-11T18:10:18.225976072Z" level=info msg="parsed scheme: \"unix\"" module=grpc
Feb 11 18:10:18 minikube dockerd[211]: time="2022-02-11T18:10:18.225992487Z" level=info msg="scheme \"unix\" not registered, fallback to default scheme" module=grpc
Feb 11 18:10:18 minikube dockerd[211]: time="2022-02-11T18:10:18.226008715Z" level=info msg="ccResolverWrapper: sending update to cc: {[{unix:///run/containerd/containerd.sock  <nil> 0 <nil>}] <nil> <nil>}" module=grpc
Feb 11 18:10:18 minikube dockerd[211]: time="2022-02-11T18:10:18.226023214Z" level=info msg="ClientConn switching balancer to \"pick_first\"" module=grpc
Feb 11 18:10:18 minikube dockerd[211]: time="2022-02-11T18:10:18.228749588Z" level=info msg="parsed scheme: \"unix\"" module=grpc
Feb 11 18:10:18 minikube dockerd[211]: time="2022-02-11T18:10:18.228762191Z" level=info msg="scheme \"unix\" not registered, fallback to default scheme" module=grpc
Feb 11 18:10:18 minikube dockerd[211]: time="2022-02-11T18:10:18.228772626Z" level=info msg="ccResolverWrapper: sending update to cc: {[{unix:///run/containerd/containerd.sock  <nil> 0 <nil>}] <nil> <nil>}" module=grpc
Feb 11 18:10:18 minikube dockerd[211]: time="2022-02-11T18:10:18.228778924Z" level=info msg="ClientConn switching balancer to \"pick_first\"" module=grpc
Feb 11 18:10:18 minikube dockerd[211]: time="2022-02-11T18:10:18.251733302Z" level=info msg="[graphdriver] using prior storage driver: overlay2"
Feb 11 18:10:18 minikube dockerd[211]: time="2022-02-11T18:10:18.329996936Z" level=warning msg="Your kernel does not support CPU realtime scheduler"
Feb 11 18:10:18 minikube dockerd[211]: time="2022-02-11T18:10:18.330016408Z" level=warning msg="Your kernel does not support cgroup blkio weight"
Feb 11 18:10:18 minikube dockerd[211]: time="2022-02-11T18:10:18.330023923Z" level=warning msg="Your kernel does not support cgroup blkio weight_device"
Feb 11 18:10:18 minikube dockerd[211]: time="2022-02-11T18:10:18.330352244Z" level=info msg="Loading containers: start."
Feb 11 18:10:18 minikube dockerd[211]: time="2022-02-11T18:10:18.474845029Z" level=info msg="Default bridge (docker0) is assigned with an IP address 172.17.0.0/16. Daemon option --bip can be used to set a preferred IP address"
Feb 11 18:10:18 minikube dockerd[211]: time="2022-02-11T18:10:18.504038862Z" level=info msg="Loading containers: done."
Feb 11 18:10:18 minikube dockerd[211]: time="2022-02-11T18:10:18.536233291Z" level=info msg="Docker daemon" commit=75249d8 graphdriver(s)=overlay2 version=20.10.8
Feb 11 18:10:18 minikube dockerd[211]: time="2022-02-11T18:10:18.537075874Z" level=info msg="Daemon has completed initialization"
Feb 11 18:10:18 minikube systemd[1]: Started Docker Application Container Engine.
Feb 11 18:10:18 minikube dockerd[211]: time="2022-02-11T18:10:18.553565271Z" level=info msg="API listen on [::]:2376"
Feb 11 18:10:18 minikube dockerd[211]: time="2022-02-11T18:10:18.556701004Z" level=info msg="API listen on /var/run/docker.sock"
Feb 11 18:10:36 minikube dockerd[211]: time="2022-02-11T18:10:36.943704768Z" level=info msg="ignoring event" container=af7b5d113b9ac93df2ae2846d640af6e1e65ad34df8d1168a67678b0143596a7 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Feb 11 18:14:17 minikube dockerd[211]: time="2022-02-11T18:14:17.459436344Z" level=warning msg="reference for unknown type: " digest="sha256:64d8c73dca984af206adf9d6d7e46aa550362b1d7a01f3a0a91b20cc67868660" remote="k8s.gcr.io/ingress-nginx/kube-webhook-certgen@sha256:64d8c73dca984af206adf9d6d7e46aa550362b1d7a01f3a0a91b20cc67868660"
Feb 11 18:14:20 minikube dockerd[211]: time="2022-02-11T18:14:20.527883764Z" level=info msg="ignoring event" container=07b7eff25d6c1f736d22d8ad1c02a12899ac468d1fc48e175b521c81cdc01785 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Feb 11 18:14:20 minikube dockerd[211]: time="2022-02-11T18:14:20.623387662Z" level=info msg="ignoring event" container=3ba522869e05e083f9784829df43ebb23d7e31738ead11566eb4f0c5c231b252 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Feb 11 18:14:21 minikube dockerd[211]: time="2022-02-11T18:14:21.576735611Z" level=info msg="ignoring event" container=757f0b3fb355bb432f8bb7f708a030bfdf2a45722594b99643be64d665f5d16d module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Feb 11 18:14:21 minikube dockerd[211]: time="2022-02-11T18:14:21.697711390Z" level=info msg="ignoring event" container=42f87977f48af0d878638df9ba7c0233d08c9e950457744bc7f774b8dbb828fc module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Feb 11 18:14:22 minikube dockerd[211]: time="2022-02-11T18:14:22.562012501Z" level=info msg="ignoring event" container=85d3670ccda24bb2cac144ecf2cdc4ac4ac27d44b7f0d7ffe5db2d187017a3e1 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Feb 11 18:14:24 minikube dockerd[211]: time="2022-02-11T18:14:24.641382570Z" level=warning msg="reference for unknown type: " digest="sha256:0bc88eb15f9e7f84e8e56c14fa5735aaa488b840983f87bd79b1054190e660de" remote="k8s.gcr.io/ingress-nginx/controller@sha256:0bc88eb15f9e7f84e8e56c14fa5735aaa488b840983f87bd79b1054190e660de"
Feb 11 18:14:57 minikube dockerd[211]: time="2022-02-11T18:14:57.561553357Z" level=warning msg="reference for unknown type: " digest="sha256:545cff00370f28363dad31e3b59a94ba377854d3a11f18988f5f9e56841ef9ef" remote="k8s.gcr.io/ingress-nginx/controller@sha256:545cff00370f28363dad31e3b59a94ba377854d3a11f18988f5f9e56841ef9ef"
Feb 11 18:15:28 minikube dockerd[211]: time="2022-02-11T18:15:28.032909314Z" level=info msg="ignoring event" container=8bf1c16c7080149fc812967bd3b5dbeb4610240e39671e516bcd939ecd11bb4f module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Feb 11 18:15:28 minikube dockerd[211]: time="2022-02-11T18:15:28.111612787Z" level=info msg="ignoring event" container=1dbc2decd712f902d147b14cb480a633e9806174d1cc553f7699543a1f3da036 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Feb 11 18:30:55 minikube dockerd[211]: time="2022-02-11T18:30:55.044168226Z" level=info msg="ignoring event" container=b1d3a4e277257cc68a630a5fd0baec47a5cd5604911d22efe6306a969fbf031b module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Feb 11 18:30:55 minikube dockerd[211]: time="2022-02-11T18:30:55.116550873Z" level=info msg="ignoring event" container=518175fba578f9860117184026899f4c3718f5d2a5aeb3dbb56eddf663724806 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Feb 11 18:32:14 minikube dockerd[211]: time="2022-02-11T18:32:14.024892963Z" level=info msg="ignoring event" container=ee41341e1abd98ade767efb3e70bafd39e183bd76a8c702113832724de8b5439 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Feb 11 18:32:14 minikube dockerd[211]: time="2022-02-11T18:32:14.101159645Z" level=info msg="ignoring event" container=f26c8afcf9e5415a4b45df2255b8ec5a75d09cf6693e9434f9abb5c57d6ff1a9 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"

* 
* ==> container status <==
* CONTAINER           IMAGE                                                                                                                   CREATED             STATE               NAME                      ATTEMPT             POD ID
7e77471f012db       a9f76bcccfb5f                                                                                                           2 minutes ago       Running             controller                0                   c0b3b82715e03
c09234940cedd       httpd@sha256:5cc947a200524a822883dc6ce6456d852d7c5629ab177dfbf7e38c1b4a647705                                           2 minutes ago       Running             httpd                     0                   34df80308cd74
42f87977f48af       c41e9fcadf5a2                                                                                                           19 minutes ago      Exited              patch                     1                   85d3670ccda24
3ba522869e05e       k8s.gcr.io/ingress-nginx/kube-webhook-certgen@sha256:64d8c73dca984af206adf9d6d7e46aa550362b1d7a01f3a0a91b20cc67868660   19 minutes ago      Exited              create                    0                   757f0b3fb355b
587a13b465df1       postgres@sha256:3162a6ead070474b27289f09eac4c865e75f93847a2d7098f718ee5a721637c4                                        22 minutes ago      Running             postgres                  1                   0e80c3804132d
1cac57b0ddb11       6e38f40d628db                                                                                                           22 minutes ago      Running             storage-provisioner       29                  90c4c089055dc
48b7a2145445e       cygnetops/multi-server-pgfix-5-11@sha256:e605f0da73915e839477782a9952c39bac3581d8863384f26abfa0f42ad9d87d               23 minutes ago      Running             server                    1                   361bce46c14c7
bbec8809ea28b       stephengrider/multi-client@sha256:855452509d6d9f13dbe1cd34fa3a21d7f6e7d1f0fafb38d1e715dda8e3d17f46                      23 minutes ago      Running             client                    4                   4abd65dd7d912
f5ca438dfa4f2       stephengrider/multi-worker@sha256:5fbab5f86e6a4d499926349a5f0ec032c42e7f7450acc98b053791df26dc4d2b                      23 minutes ago      Running             worker                    1                   2f75993f0315d
7f171fe95edd6       cygnetops/multi-server-pgfix-5-11@sha256:e605f0da73915e839477782a9952c39bac3581d8863384f26abfa0f42ad9d87d               23 minutes ago      Running             server                    1                   c3f894c8aa21f
44e27949f5b53       redis@sha256:0d9c9aed1eb385336db0bc9b976b6b49774aee3d2b9c2788a0d0d9e239986cb3                                           23 minutes ago      Running             redis                     4                   f2aaa118b9eac
6a47823d5fd2b       cygnetops/multi-server-pgfix-5-11@sha256:e605f0da73915e839477782a9952c39bac3581d8863384f26abfa0f42ad9d87d               23 minutes ago      Running             server                    1                   48ef35967886c
a6e629b40a0d9       stephengrider/multi-client@sha256:855452509d6d9f13dbe1cd34fa3a21d7f6e7d1f0fafb38d1e715dda8e3d17f46                      23 minutes ago      Running             client                    4                   213d9636f8d98
7642439f0e103       stephengrider/multi-client@sha256:855452509d6d9f13dbe1cd34fa3a21d7f6e7d1f0fafb38d1e715dda8e3d17f46                      23 minutes ago      Running             client                    4                   dbade666049eb
9b57892145de7       6120bd723dced                                                                                                           23 minutes ago      Running             kube-proxy                11                  3285df9d45001
31eb53a7cc1db       8d147537fb7d1                                                                                                           23 minutes ago      Running             coredns                   19                  9d801770ab6a5
af7b5d113b9ac       6e38f40d628db                                                                                                           23 minutes ago      Exited              storage-provisioner       28                  90c4c089055dc
14d28460cbb9c       0048118155842                                                                                                           23 minutes ago      Running             etcd                      11                  d5b40d8528ae5
673547813dcc4       53224b502ea4d                                                                                                           23 minutes ago      Running             kube-apiserver            11                  1f971ab399138
2bd544b0dbdd0       0aa9c7e31d307                                                                                                           23 minutes ago      Running             kube-scheduler            11                  9422c01a9ee1e
3c9c8b02aea2d       05c905cef780c                                                                                                           23 minutes ago      Running             kube-controller-manager   11                  8fe8e292711b4
ae9cc334968b9       cygnetops/multi-server-pgfix-5-11@sha256:e605f0da73915e839477782a9952c39bac3581d8863384f26abfa0f42ad9d87d               2 days ago          Exited              server                    0                   9da7196c9f51a
570aa7942a813       cygnetops/multi-server-pgfix-5-11@sha256:e605f0da73915e839477782a9952c39bac3581d8863384f26abfa0f42ad9d87d               2 days ago          Exited              server                    0                   dc8faac3a6aa7
56d6c344f99e0       postgres@sha256:3162a6ead070474b27289f09eac4c865e75f93847a2d7098f718ee5a721637c4                                        2 days ago          Exited              postgres                  0                   b1744f683ae3b
9f27bb9ab2dbf       stephengrider/multi-worker@sha256:5fbab5f86e6a4d499926349a5f0ec032c42e7f7450acc98b053791df26dc4d2b                      2 days ago          Exited              worker                    0                   2095c8c2ac476
7055e3cee4be8       cygnetops/multi-server-pgfix-5-11@sha256:e605f0da73915e839477782a9952c39bac3581d8863384f26abfa0f42ad9d87d               2 days ago          Exited              server                    0                   b3db5e3bd5102
c904b4acd2807       redis@sha256:0d9c9aed1eb385336db0bc9b976b6b49774aee3d2b9c2788a0d0d9e239986cb3                                           2 days ago          Exited              redis                     3                   e6361051992df
054fb10b88772       stephengrider/multi-client@sha256:855452509d6d9f13dbe1cd34fa3a21d7f6e7d1f0fafb38d1e715dda8e3d17f46                      2 days ago          Exited              client                    3                   77e480650b19f
1857fde35470a       stephengrider/multi-client@sha256:855452509d6d9f13dbe1cd34fa3a21d7f6e7d1f0fafb38d1e715dda8e3d17f46                      2 days ago          Exited              client                    3                   3c8e7c4746681
821a6243ac9c9       8d147537fb7d1                                                                                                           2 days ago          Exited              coredns                   18                  8fa8a2d96f972
da745878ebe60       stephengrider/multi-client@sha256:855452509d6d9f13dbe1cd34fa3a21d7f6e7d1f0fafb38d1e715dda8e3d17f46                      2 days ago          Exited              client                    3                   b05d74873c413
35f61d1bf74b8       6120bd723dced                                                                                                           2 days ago          Exited              kube-proxy                10                  0efc5afdfa114
3229ea58fd906       53224b502ea4d                                                                                                           2 days ago          Exited              kube-apiserver            10                  93d9313e954ef
63b4e95072d70       05c905cef780c                                                                                                           2 days ago          Exited              kube-controller-manager   10                  0b2c83b5a88d8
bba2afb05bcaa       0048118155842                                                                                                           2 days ago          Exited              etcd                      10                  b33a782ac2190
0169b6e51edf3       0aa9c7e31d307                                                                                                           2 days ago          Exited              kube-scheduler            10                  9dac9d6cd7772

* 
* ==> coredns [31eb53a7cc1d] <==
* [INFO] plugin/ready: Still waiting on: "kubernetes"
.:53
[INFO] plugin/reload: Running configuration MD5 = cec3c60eb1cc4909fd4579a8d79ea031
CoreDNS-1.8.4
linux/amd64, go1.16.4, 053c4d5

* 
* ==> coredns [821a6243ac9c] <==
* .:53
[INFO] plugin/reload: Running configuration MD5 = cec3c60eb1cc4909fd4579a8d79ea031
CoreDNS-1.8.4
linux/amd64, go1.16.4, 053c4d5
[INFO] SIGTERM: Shutting down servers then terminating
[INFO] plugin/health: Going into lameduck mode for 5s

* 
* ==> describe nodes <==
* Name:               minikube
Roles:              control-plane,master
Labels:             beta.kubernetes.io/arch=amd64
                    beta.kubernetes.io/os=linux
                    kubernetes.io/arch=amd64
                    kubernetes.io/hostname=minikube
                    kubernetes.io/os=linux
                    minikube.k8s.io/commit=76b94fb3c4e8ac5062daf70d60cf03ddcc0a741b
                    minikube.k8s.io/name=minikube
                    minikube.k8s.io/updated_at=2022_01_16T12_53_49_0700
                    minikube.k8s.io/version=v1.24.0
                    node-role.kubernetes.io/control-plane=
                    node-role.kubernetes.io/master=
                    node.kubernetes.io/exclude-from-external-load-balancers=
Annotations:        kubeadm.alpha.kubernetes.io/cri-socket: /var/run/dockershim.sock
                    node.alpha.kubernetes.io/ttl: 0
                    volumes.kubernetes.io/controller-managed-attach-detach: true
CreationTimestamp:  Sun, 16 Jan 2022 12:53:45 +0000
Taints:             <none>
Unschedulable:      false
Lease:
  HolderIdentity:  minikube
  AcquireTime:     <unset>
  RenewTime:       Fri, 11 Feb 2022 18:33:41 +0000
Conditions:
  Type             Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message
  ----             ------  -----------------                 ------------------                ------                       -------
  MemoryPressure   False   Fri, 11 Feb 2022 18:31:29 +0000   Sun, 16 Jan 2022 20:59:39 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available
  DiskPressure     False   Fri, 11 Feb 2022 18:31:29 +0000   Sun, 16 Jan 2022 20:59:39 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure
  PIDPressure      False   Fri, 11 Feb 2022 18:31:29 +0000   Sun, 16 Jan 2022 20:59:39 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available
  Ready            True    Fri, 11 Feb 2022 18:31:29 +0000   Sun, 16 Jan 2022 20:59:39 +0000   KubeletReady                 kubelet is posting ready status
Addresses:
  InternalIP:  192.168.49.2
  Hostname:    minikube
Capacity:
  cpu:                12
  ephemeral-storage:  488949800Ki
  hugepages-1Gi:      0
  hugepages-2Mi:      0
  memory:             15996328Ki
  pods:               110
Allocatable:
  cpu:                12
  ephemeral-storage:  488949800Ki
  hugepages-1Gi:      0
  hugepages-2Mi:      0
  memory:             15996328Ki
  pods:               110
System Info:
  Machine ID:                 bba0be70c47c400ea3cf7733f1c0b4c1
  System UUID:                431926c4-9733-49d7-bad9-73beb5ddc353
  Boot ID:                    ba05e295-5c7c-42c6-8960-44be0dfc3563
  Kernel Version:             5.13.0-28-generic
  OS Image:                   Ubuntu 20.04.2 LTS
  Operating System:           linux
  Architecture:               amd64
  Container Runtime Version:  docker://20.10.8
  Kubelet Version:            v1.22.3
  Kube-Proxy Version:         v1.22.3
PodCIDR:                      10.244.0.0/24
PodCIDRs:                     10.244.0.0/24
Non-terminated Pods:          (18 in total)
  Namespace                   Name                                         CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age
  ---------                   ----                                         ------------  ----------  ---------------  -------------  ---
  default                     client-deployment-7cb6c958f7-2css8           0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         11d
  default                     client-deployment-7cb6c958f7-gdk5r           0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         11d
  default                     client-deployment-7cb6c958f7-znrwf           0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         11d
  default                     demo-654c477f6d-hrldp                        0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         2m32s
  default                     postgres-deployment-5b7fdb4969-hk6td         0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         47h
  default                     redis-deployment-58c4799ccc-nxmbj            0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         10d
  default                     server-deployment-58cf857974-56vdn           0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         47h
  default                     server-deployment-58cf857974-7zmmp           0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         47h
  default                     server-deployment-58cf857974-d2twr           0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         47h
  default                     worker-deployment-7c94ff9b64-z978x           0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         47h
  ingress-nginx               ingress-nginx-controller-5f66978484-9c6zp    100m (0%!)(MISSING)     0 (0%!)(MISSING)      90Mi (0%!)(MISSING)        0 (0%!)(MISSING)         2m6s
  kube-system                 coredns-78fcd69978-zszqt                     100m (0%!)(MISSING)     0 (0%!)(MISSING)      70Mi (0%!)(MISSING)        170Mi (1%!)(MISSING)     26d
  kube-system                 etcd-minikube                                100m (0%!)(MISSING)     0 (0%!)(MISSING)      100Mi (0%!)(MISSING)       0 (0%!)(MISSING)         26d
  kube-system                 kube-apiserver-minikube                      250m (2%!)(MISSING)     0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         26d
  kube-system                 kube-controller-manager-minikube             200m (1%!)(MISSING)     0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         26d
  kube-system                 kube-proxy-cp82l                             0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         26d
  kube-system                 kube-scheduler-minikube                      100m (0%!)(MISSING)     0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         26d
  kube-system                 storage-provisioner                          0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         26d
Allocated resources:
  (Total limits may be over 100 percent, i.e., overcommitted.)
  Resource           Requests    Limits
  --------           --------    ------
  cpu                850m (7%!)(MISSING)   0 (0%!)(MISSING)
  memory             260Mi (1%!)(MISSING)  170Mi (1%!)(MISSING)
  ephemeral-storage  0 (0%!)(MISSING)      0 (0%!)(MISSING)
  hugepages-1Gi      0 (0%!)(MISSING)      0 (0%!)(MISSING)
  hugepages-2Mi      0 (0%!)(MISSING)      0 (0%!)(MISSING)
Events:
  Type    Reason                   Age                    From        Message
  ----    ------                   ----                   ----        -------
  Normal  Starting                 3d23h                  kube-proxy  
  Normal  Starting                 2d                     kube-proxy  
  Normal  Starting                 23m                    kube-proxy  
  Normal  Starting                 3d23h                  kubelet     Starting kubelet.
  Normal  NodeHasNoDiskPressure    3d23h (x8 over 3d23h)  kubelet     Node minikube status is now: NodeHasNoDiskPressure
  Normal  NodeHasSufficientPID     3d23h (x7 over 3d23h)  kubelet     Node minikube status is now: NodeHasSufficientPID
  Normal  NodeHasSufficientMemory  3d23h (x8 over 3d23h)  kubelet     Node minikube status is now: NodeHasSufficientMemory
  Normal  NodeAllocatableEnforced  3d23h                  kubelet     Updated Node Allocatable limit across pods
  Normal  NodeHasNoDiskPressure    2d (x8 over 2d)        kubelet     Node minikube status is now: NodeHasNoDiskPressure
  Normal  NodeHasSufficientMemory  2d (x8 over 2d)        kubelet     Node minikube status is now: NodeHasSufficientMemory
  Normal  NodeAllocatableEnforced  2d                     kubelet     Updated Node Allocatable limit across pods
  Normal  Starting                 2d                     kubelet     Starting kubelet.
  Normal  NodeHasSufficientPID     2d (x7 over 2d)        kubelet     Node minikube status is now: NodeHasSufficientPID
  Normal  Starting                 23m                    kubelet     Starting kubelet.
  Normal  NodeAllocatableEnforced  23m                    kubelet     Updated Node Allocatable limit across pods
  Normal  NodeHasSufficientMemory  23m (x8 over 23m)      kubelet     Node minikube status is now: NodeHasSufficientMemory
  Normal  NodeHasNoDiskPressure    23m (x8 over 23m)      kubelet     Node minikube status is now: NodeHasNoDiskPressure
  Normal  NodeHasSufficientPID     23m (x7 over 23m)      kubelet     Node minikube status is now: NodeHasSufficientPID

* 
* ==> dmesg <==
* 
* 
* ==> etcd [14d28460cbb9] <==
* {"level":"info","ts":"2022-02-11T18:10:28.878Z","caller":"etcdmain/etcd.go:72","msg":"Running: ","args":["etcd","--advertise-client-urls=https://192.168.49.2:2379","--cert-file=/var/lib/minikube/certs/etcd/server.crt","--client-cert-auth=true","--data-dir=/var/lib/minikube/etcd","--initial-advertise-peer-urls=https://192.168.49.2:2380","--initial-cluster=minikube=https://192.168.49.2:2380","--key-file=/var/lib/minikube/certs/etcd/server.key","--listen-client-urls=https://127.0.0.1:2379,https://192.168.49.2:2379","--listen-metrics-urls=http://127.0.0.1:2381","--listen-peer-urls=https://192.168.49.2:2380","--name=minikube","--peer-cert-file=/var/lib/minikube/certs/etcd/peer.crt","--peer-client-cert-auth=true","--peer-key-file=/var/lib/minikube/certs/etcd/peer.key","--peer-trusted-ca-file=/var/lib/minikube/certs/etcd/ca.crt","--proxy-refresh-interval=70000","--snapshot-count=10000","--trusted-ca-file=/var/lib/minikube/certs/etcd/ca.crt"]}
{"level":"info","ts":"2022-02-11T18:10:28.878Z","caller":"etcdmain/etcd.go:115","msg":"server has been already initialized","data-dir":"/var/lib/minikube/etcd","dir-type":"member"}
{"level":"info","ts":"2022-02-11T18:10:28.878Z","caller":"embed/etcd.go:131","msg":"configuring peer listeners","listen-peer-urls":["https://192.168.49.2:2380"]}
{"level":"info","ts":"2022-02-11T18:10:28.878Z","caller":"embed/etcd.go:478","msg":"starting with peer TLS","tls-info":"cert = /var/lib/minikube/certs/etcd/peer.crt, key = /var/lib/minikube/certs/etcd/peer.key, client-cert=, client-key=, trusted-ca = /var/lib/minikube/certs/etcd/ca.crt, client-cert-auth = true, crl-file = ","cipher-suites":[]}
{"level":"info","ts":"2022-02-11T18:10:28.879Z","caller":"embed/etcd.go:139","msg":"configuring client listeners","listen-client-urls":["https://127.0.0.1:2379","https://192.168.49.2:2379"]}
{"level":"info","ts":"2022-02-11T18:10:28.880Z","caller":"embed/etcd.go:307","msg":"starting an etcd server","etcd-version":"3.5.0","git-sha":"946a5a6f2","go-version":"go1.16.3","go-os":"linux","go-arch":"amd64","max-cpu-set":12,"max-cpu-available":12,"member-initialized":true,"name":"minikube","data-dir":"/var/lib/minikube/etcd","wal-dir":"","wal-dir-dedicated":"","member-dir":"/var/lib/minikube/etcd/member","force-new-cluster":false,"heartbeat-interval":"100ms","election-timeout":"1s","initial-election-tick-advance":true,"snapshot-count":10000,"snapshot-catchup-entries":5000,"initial-advertise-peer-urls":["https://192.168.49.2:2380"],"listen-peer-urls":["https://192.168.49.2:2380"],"advertise-client-urls":["https://192.168.49.2:2379"],"listen-client-urls":["https://127.0.0.1:2379","https://192.168.49.2:2379"],"listen-metrics-urls":["http://127.0.0.1:2381"],"cors":["*"],"host-whitelist":["*"],"initial-cluster":"","initial-cluster-state":"new","initial-cluster-token":"","quota-size-bytes":2147483648,"pre-vote":true,"initial-corrupt-check":false,"corrupt-check-time-interval":"0s","auto-compaction-mode":"periodic","auto-compaction-retention":"0s","auto-compaction-interval":"0s","discovery-url":"","discovery-proxy":"","downgrade-check-interval":"5s"}
{"level":"info","ts":"2022-02-11T18:10:28.892Z","caller":"etcdserver/backend.go:81","msg":"opened backend db","path":"/var/lib/minikube/etcd/member/snap/db","took":"11.934427ms"}
{"level":"info","ts":"2022-02-11T18:10:30.962Z","caller":"etcdserver/server.go:508","msg":"recovered v2 store from snapshot","snapshot-index":130013,"snapshot-size":"10 kB"}
{"level":"info","ts":"2022-02-11T18:10:30.962Z","caller":"etcdserver/server.go:518","msg":"recovered v3 backend from snapshot","backend-size-bytes":3575808,"backend-size":"3.6 MB","backend-size-in-use-bytes":2449408,"backend-size-in-use":"2.4 MB"}
{"level":"info","ts":"2022-02-11T18:10:31.385Z","caller":"etcdserver/raft.go:483","msg":"restarting local member","cluster-id":"fa54960ea34d58be","local-member-id":"aec36adc501070cc","commit-index":134789}
{"level":"info","ts":"2022-02-11T18:10:31.451Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc switched to configuration voters=(12593026477526642892)"}
{"level":"info","ts":"2022-02-11T18:10:31.451Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc became follower at term 12"}
{"level":"info","ts":"2022-02-11T18:10:31.451Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"newRaft aec36adc501070cc [peers: [aec36adc501070cc], term: 12, commit: 134789, applied: 130013, lastindex: 134789, lastterm: 12]"}
{"level":"info","ts":"2022-02-11T18:10:31.451Z","caller":"api/capability.go:75","msg":"enabled capabilities for version","cluster-version":"3.5"}
{"level":"info","ts":"2022-02-11T18:10:31.451Z","caller":"membership/cluster.go:276","msg":"recovered/added member from store","cluster-id":"fa54960ea34d58be","local-member-id":"aec36adc501070cc","recovered-remote-peer-id":"aec36adc501070cc","recovered-remote-peer-urls":["https://192.168.49.2:2380"]}
{"level":"info","ts":"2022-02-11T18:10:31.451Z","caller":"membership/cluster.go:285","msg":"set cluster version from store","cluster-version":"3.5"}
{"level":"warn","ts":"2022-02-11T18:10:31.452Z","caller":"auth/store.go:1220","msg":"simple token is not cryptographically signed"}
{"level":"info","ts":"2022-02-11T18:10:31.452Z","caller":"mvcc/kvstore.go:345","msg":"restored last compact revision","meta-bucket-name":"meta","meta-bucket-name-key":"finishedCompactRev","restored-compact-revision":104293}
{"level":"info","ts":"2022-02-11T18:10:31.455Z","caller":"mvcc/kvstore.go:415","msg":"kvstore restored","current-rev":104878}
{"level":"info","ts":"2022-02-11T18:10:31.456Z","caller":"etcdserver/quota.go:94","msg":"enabled backend quota with default value","quota-name":"v3-applier","quota-size-bytes":2147483648,"quota-size":"2.1 GB"}
{"level":"info","ts":"2022-02-11T18:10:31.457Z","caller":"etcdserver/server.go:834","msg":"starting etcd server","local-member-id":"aec36adc501070cc","local-server-version":"3.5.0","cluster-id":"fa54960ea34d58be","cluster-version":"3.5"}
{"level":"info","ts":"2022-02-11T18:10:31.457Z","caller":"etcdserver/server.go:728","msg":"started as single-node; fast-forwarding election ticks","local-member-id":"aec36adc501070cc","forward-ticks":9,"forward-duration":"900ms","election-ticks":10,"election-timeout":"1s"}
{"level":"info","ts":"2022-02-11T18:10:31.459Z","caller":"embed/etcd.go:687","msg":"starting with client TLS","tls-info":"cert = /var/lib/minikube/certs/etcd/server.crt, key = /var/lib/minikube/certs/etcd/server.key, client-cert=, client-key=, trusted-ca = /var/lib/minikube/certs/etcd/ca.crt, client-cert-auth = true, crl-file = ","cipher-suites":[]}
{"level":"info","ts":"2022-02-11T18:10:31.460Z","caller":"embed/etcd.go:580","msg":"serving peer traffic","address":"192.168.49.2:2380"}
{"level":"info","ts":"2022-02-11T18:10:31.460Z","caller":"embed/etcd.go:552","msg":"cmux::serve","address":"192.168.49.2:2380"}
{"level":"info","ts":"2022-02-11T18:10:31.460Z","caller":"embed/etcd.go:276","msg":"now serving peer/client/metrics","local-member-id":"aec36adc501070cc","initial-advertise-peer-urls":["https://192.168.49.2:2380"],"listen-peer-urls":["https://192.168.49.2:2380"],"advertise-client-urls":["https://192.168.49.2:2379"],"listen-client-urls":["https://127.0.0.1:2379","https://192.168.49.2:2379"],"listen-metrics-urls":["http://127.0.0.1:2381"]}
{"level":"info","ts":"2022-02-11T18:10:31.460Z","caller":"embed/etcd.go:762","msg":"serving metrics","address":"http://127.0.0.1:2381"}
{"level":"info","ts":"2022-02-11T18:10:32.252Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc is starting a new election at term 12"}
{"level":"info","ts":"2022-02-11T18:10:32.252Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc became pre-candidate at term 12"}
{"level":"info","ts":"2022-02-11T18:10:32.252Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc received MsgPreVoteResp from aec36adc501070cc at term 12"}
{"level":"info","ts":"2022-02-11T18:10:32.252Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc became candidate at term 13"}
{"level":"info","ts":"2022-02-11T18:10:32.252Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc received MsgVoteResp from aec36adc501070cc at term 13"}
{"level":"info","ts":"2022-02-11T18:10:32.252Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc became leader at term 13"}
{"level":"info","ts":"2022-02-11T18:10:32.252Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"raft.node: aec36adc501070cc elected leader aec36adc501070cc at term 13"}
{"level":"info","ts":"2022-02-11T18:10:32.252Z","caller":"etcdserver/server.go:2027","msg":"published local member to cluster through raft","local-member-id":"aec36adc501070cc","local-member-attributes":"{Name:minikube ClientURLs:[https://192.168.49.2:2379]}","request-path":"/0/members/aec36adc501070cc/attributes","cluster-id":"fa54960ea34d58be","publish-timeout":"7s"}
{"level":"info","ts":"2022-02-11T18:10:32.252Z","caller":"embed/serve.go:98","msg":"ready to serve client requests"}
{"level":"info","ts":"2022-02-11T18:10:32.252Z","caller":"embed/serve.go:98","msg":"ready to serve client requests"}
{"level":"info","ts":"2022-02-11T18:10:32.252Z","caller":"etcdmain/main.go:47","msg":"notifying init daemon"}
{"level":"info","ts":"2022-02-11T18:10:32.252Z","caller":"etcdmain/main.go:53","msg":"successfully notified init daemon"}
{"level":"info","ts":"2022-02-11T18:10:32.253Z","caller":"embed/serve.go:188","msg":"serving client traffic securely","address":"127.0.0.1:2379"}
{"level":"info","ts":"2022-02-11T18:10:32.253Z","caller":"embed/serve.go:188","msg":"serving client traffic securely","address":"192.168.49.2:2379"}
{"level":"info","ts":"2022-02-11T18:20:32.266Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":105474}
{"level":"info","ts":"2022-02-11T18:20:32.283Z","caller":"mvcc/kvstore_compaction.go:57","msg":"finished scheduled compaction","compact-revision":105474,"took":"16.645261ms"}
{"level":"info","ts":"2022-02-11T18:25:32.271Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":105719}
{"level":"info","ts":"2022-02-11T18:25:32.285Z","caller":"mvcc/kvstore_compaction.go:57","msg":"finished scheduled compaction","compact-revision":105719,"took":"13.323899ms"}
{"level":"info","ts":"2022-02-11T18:26:00.191Z","caller":"wal/wal.go:782","msg":"created a new WAL segment","path":"/var/lib/minikube/etcd/member/wal/0000000000000002-00000000000213b1.wal"}
{"level":"info","ts":"2022-02-11T18:30:32.276Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":105969}
{"level":"info","ts":"2022-02-11T18:30:32.288Z","caller":"mvcc/kvstore_compaction.go:57","msg":"finished scheduled compaction","compact-revision":105969,"took":"12.24548ms"}

* 
* ==> etcd [bba2afb05bca] <==
* {"level":"info","ts":"2022-02-09T18:20:54.625Z","caller":"etcdmain/etcd.go:72","msg":"Running: ","args":["etcd","--advertise-client-urls=https://192.168.49.2:2379","--cert-file=/var/lib/minikube/certs/etcd/server.crt","--client-cert-auth=true","--data-dir=/var/lib/minikube/etcd","--initial-advertise-peer-urls=https://192.168.49.2:2380","--initial-cluster=minikube=https://192.168.49.2:2380","--key-file=/var/lib/minikube/certs/etcd/server.key","--listen-client-urls=https://127.0.0.1:2379,https://192.168.49.2:2379","--listen-metrics-urls=http://127.0.0.1:2381","--listen-peer-urls=https://192.168.49.2:2380","--name=minikube","--peer-cert-file=/var/lib/minikube/certs/etcd/peer.crt","--peer-client-cert-auth=true","--peer-key-file=/var/lib/minikube/certs/etcd/peer.key","--peer-trusted-ca-file=/var/lib/minikube/certs/etcd/ca.crt","--proxy-refresh-interval=70000","--snapshot-count=10000","--trusted-ca-file=/var/lib/minikube/certs/etcd/ca.crt"]}
{"level":"info","ts":"2022-02-09T18:20:54.626Z","caller":"etcdmain/etcd.go:115","msg":"server has been already initialized","data-dir":"/var/lib/minikube/etcd","dir-type":"member"}
{"level":"info","ts":"2022-02-09T18:20:54.626Z","caller":"embed/etcd.go:131","msg":"configuring peer listeners","listen-peer-urls":["https://192.168.49.2:2380"]}
{"level":"info","ts":"2022-02-09T18:20:54.626Z","caller":"embed/etcd.go:478","msg":"starting with peer TLS","tls-info":"cert = /var/lib/minikube/certs/etcd/peer.crt, key = /var/lib/minikube/certs/etcd/peer.key, client-cert=, client-key=, trusted-ca = /var/lib/minikube/certs/etcd/ca.crt, client-cert-auth = true, crl-file = ","cipher-suites":[]}
{"level":"info","ts":"2022-02-09T18:20:54.627Z","caller":"embed/etcd.go:139","msg":"configuring client listeners","listen-client-urls":["https://127.0.0.1:2379","https://192.168.49.2:2379"]}
{"level":"info","ts":"2022-02-09T18:20:54.627Z","caller":"embed/etcd.go:307","msg":"starting an etcd server","etcd-version":"3.5.0","git-sha":"946a5a6f2","go-version":"go1.16.3","go-os":"linux","go-arch":"amd64","max-cpu-set":12,"max-cpu-available":12,"member-initialized":true,"name":"minikube","data-dir":"/var/lib/minikube/etcd","wal-dir":"","wal-dir-dedicated":"","member-dir":"/var/lib/minikube/etcd/member","force-new-cluster":false,"heartbeat-interval":"100ms","election-timeout":"1s","initial-election-tick-advance":true,"snapshot-count":10000,"snapshot-catchup-entries":5000,"initial-advertise-peer-urls":["https://192.168.49.2:2380"],"listen-peer-urls":["https://192.168.49.2:2380"],"advertise-client-urls":["https://192.168.49.2:2379"],"listen-client-urls":["https://127.0.0.1:2379","https://192.168.49.2:2379"],"listen-metrics-urls":["http://127.0.0.1:2381"],"cors":["*"],"host-whitelist":["*"],"initial-cluster":"","initial-cluster-state":"new","initial-cluster-token":"","quota-size-bytes":2147483648,"pre-vote":true,"initial-corrupt-check":false,"corrupt-check-time-interval":"0s","auto-compaction-mode":"periodic","auto-compaction-retention":"0s","auto-compaction-interval":"0s","discovery-url":"","discovery-proxy":"","downgrade-check-interval":"5s"}
{"level":"info","ts":"2022-02-09T18:20:54.639Z","caller":"etcdserver/backend.go:81","msg":"opened backend db","path":"/var/lib/minikube/etcd/member/snap/db","took":"10.831901ms"}
{"level":"info","ts":"2022-02-09T18:20:55.555Z","caller":"etcdserver/server.go:508","msg":"recovered v2 store from snapshot","snapshot-index":130013,"snapshot-size":"10 kB"}
{"level":"info","ts":"2022-02-09T18:20:55.555Z","caller":"etcdserver/server.go:518","msg":"recovered v3 backend from snapshot","backend-size-bytes":3575808,"backend-size":"3.6 MB","backend-size-in-use-bytes":1458176,"backend-size-in-use":"1.5 MB"}
{"level":"info","ts":"2022-02-09T18:20:55.876Z","caller":"etcdserver/raft.go:483","msg":"restarting local member","cluster-id":"fa54960ea34d58be","local-member-id":"aec36adc501070cc","commit-index":132447}
{"level":"info","ts":"2022-02-09T18:20:55.937Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc switched to configuration voters=(12593026477526642892)"}
{"level":"info","ts":"2022-02-09T18:20:55.937Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc became follower at term 11"}
{"level":"info","ts":"2022-02-09T18:20:55.937Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"newRaft aec36adc501070cc [peers: [aec36adc501070cc], term: 11, commit: 132447, applied: 130013, lastindex: 132447, lastterm: 11]"}
{"level":"info","ts":"2022-02-09T18:20:55.938Z","caller":"api/capability.go:75","msg":"enabled capabilities for version","cluster-version":"3.5"}
{"level":"info","ts":"2022-02-09T18:20:55.938Z","caller":"membership/cluster.go:276","msg":"recovered/added member from store","cluster-id":"fa54960ea34d58be","local-member-id":"aec36adc501070cc","recovered-remote-peer-id":"aec36adc501070cc","recovered-remote-peer-urls":["https://192.168.49.2:2380"]}
{"level":"info","ts":"2022-02-09T18:20:55.938Z","caller":"membership/cluster.go:285","msg":"set cluster version from store","cluster-version":"3.5"}
{"level":"warn","ts":"2022-02-09T18:20:55.939Z","caller":"auth/store.go:1220","msg":"simple token is not cryptographically signed"}
{"level":"info","ts":"2022-02-09T18:20:55.940Z","caller":"mvcc/kvstore.go:345","msg":"restored last compact revision","meta-bucket-name":"meta","meta-bucket-name-key":"finishedCompactRev","restored-compact-revision":102733}
{"level":"info","ts":"2022-02-09T18:20:55.944Z","caller":"mvcc/kvstore.go:415","msg":"kvstore restored","current-rev":102981}
{"level":"info","ts":"2022-02-09T18:20:55.945Z","caller":"etcdserver/quota.go:94","msg":"enabled backend quota with default value","quota-name":"v3-applier","quota-size-bytes":2147483648,"quota-size":"2.1 GB"}
{"level":"info","ts":"2022-02-09T18:20:55.947Z","caller":"etcdserver/server.go:834","msg":"starting etcd server","local-member-id":"aec36adc501070cc","local-server-version":"3.5.0","cluster-id":"fa54960ea34d58be","cluster-version":"3.5"}
{"level":"info","ts":"2022-02-09T18:20:55.947Z","caller":"etcdserver/server.go:728","msg":"started as single-node; fast-forwarding election ticks","local-member-id":"aec36adc501070cc","forward-ticks":9,"forward-duration":"900ms","election-ticks":10,"election-timeout":"1s"}
{"level":"info","ts":"2022-02-09T18:20:55.950Z","caller":"embed/etcd.go:687","msg":"starting with client TLS","tls-info":"cert = /var/lib/minikube/certs/etcd/server.crt, key = /var/lib/minikube/certs/etcd/server.key, client-cert=, client-key=, trusted-ca = /var/lib/minikube/certs/etcd/ca.crt, client-cert-auth = true, crl-file = ","cipher-suites":[]}
{"level":"info","ts":"2022-02-09T18:20:55.950Z","caller":"embed/etcd.go:276","msg":"now serving peer/client/metrics","local-member-id":"aec36adc501070cc","initial-advertise-peer-urls":["https://192.168.49.2:2380"],"listen-peer-urls":["https://192.168.49.2:2380"],"advertise-client-urls":["https://192.168.49.2:2379"],"listen-client-urls":["https://127.0.0.1:2379","https://192.168.49.2:2379"],"listen-metrics-urls":["http://127.0.0.1:2381"]}
{"level":"info","ts":"2022-02-09T18:20:55.950Z","caller":"embed/etcd.go:580","msg":"serving peer traffic","address":"192.168.49.2:2380"}
{"level":"info","ts":"2022-02-09T18:20:55.950Z","caller":"embed/etcd.go:552","msg":"cmux::serve","address":"192.168.49.2:2380"}
{"level":"info","ts":"2022-02-09T18:20:55.951Z","caller":"embed/etcd.go:762","msg":"serving metrics","address":"http://127.0.0.1:2381"}
{"level":"info","ts":"2022-02-09T18:20:56.638Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc is starting a new election at term 11"}
{"level":"info","ts":"2022-02-09T18:20:56.638Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc became pre-candidate at term 11"}
{"level":"info","ts":"2022-02-09T18:20:56.638Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc received MsgPreVoteResp from aec36adc501070cc at term 11"}
{"level":"info","ts":"2022-02-09T18:20:56.638Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc became candidate at term 12"}
{"level":"info","ts":"2022-02-09T18:20:56.638Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc received MsgVoteResp from aec36adc501070cc at term 12"}
{"level":"info","ts":"2022-02-09T18:20:56.638Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc became leader at term 12"}
{"level":"info","ts":"2022-02-09T18:20:56.638Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"raft.node: aec36adc501070cc elected leader aec36adc501070cc at term 12"}
{"level":"info","ts":"2022-02-09T18:20:56.639Z","caller":"etcdserver/server.go:2027","msg":"published local member to cluster through raft","local-member-id":"aec36adc501070cc","local-member-attributes":"{Name:minikube ClientURLs:[https://192.168.49.2:2379]}","request-path":"/0/members/aec36adc501070cc/attributes","cluster-id":"fa54960ea34d58be","publish-timeout":"7s"}
{"level":"info","ts":"2022-02-09T18:20:56.639Z","caller":"embed/serve.go:98","msg":"ready to serve client requests"}
{"level":"info","ts":"2022-02-09T18:20:56.639Z","caller":"embed/serve.go:98","msg":"ready to serve client requests"}
{"level":"info","ts":"2022-02-09T18:20:56.639Z","caller":"etcdmain/main.go:47","msg":"notifying init daemon"}
{"level":"info","ts":"2022-02-09T18:20:56.639Z","caller":"etcdmain/main.go:53","msg":"successfully notified init daemon"}
{"level":"info","ts":"2022-02-09T18:20:56.640Z","caller":"embed/serve.go:188","msg":"serving client traffic securely","address":"127.0.0.1:2379"}
{"level":"info","ts":"2022-02-09T18:20:56.640Z","caller":"embed/serve.go:188","msg":"serving client traffic securely","address":"192.168.49.2:2379"}
{"level":"info","ts":"2022-02-09T18:30:56.653Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":103429}
{"level":"info","ts":"2022-02-09T18:30:56.669Z","caller":"mvcc/kvstore_compaction.go:57","msg":"finished scheduled compaction","compact-revision":103429,"took":"15.167235ms"}
{"level":"info","ts":"2022-02-09T18:35:56.659Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":103645}
{"level":"info","ts":"2022-02-09T18:35:56.659Z","caller":"mvcc/kvstore_compaction.go:57","msg":"finished scheduled compaction","compact-revision":103645,"took":"374.152µs"}
{"level":"info","ts":"2022-02-09T18:40:56.664Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":103861}
{"level":"info","ts":"2022-02-09T18:40:56.665Z","caller":"mvcc/kvstore_compaction.go:57","msg":"finished scheduled compaction","compact-revision":103861,"took":"441.338µs"}
{"level":"info","ts":"2022-02-09T18:45:56.671Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":104076}
{"level":"info","ts":"2022-02-09T18:45:56.671Z","caller":"mvcc/kvstore_compaction.go:57","msg":"finished scheduled compaction","compact-revision":104076,"took":"406.768µs"}
{"level":"info","ts":"2022-02-09T18:50:56.676Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":104293}
{"level":"info","ts":"2022-02-09T18:50:56.677Z","caller":"mvcc/kvstore_compaction.go:57","msg":"finished scheduled compaction","compact-revision":104293,"took":"501.55µs"}
{"level":"info","ts":"2022-02-09T18:55:07.652Z","caller":"osutil/interrupt_unix.go:64","msg":"received signal; shutting down","signal":"terminated"}
{"level":"info","ts":"2022-02-09T18:55:07.652Z","caller":"embed/etcd.go:367","msg":"closing etcd server","name":"minikube","data-dir":"/var/lib/minikube/etcd","advertise-peer-urls":["https://192.168.49.2:2380"],"advertise-client-urls":["https://192.168.49.2:2379"]}
WARNING: 2022/02/09 18:55:07 [core] grpc: addrConn.createTransport failed to connect to {192.168.49.2:2379 192.168.49.2:2379 <nil> 0 <nil>}. Err: connection error: desc = "transport: Error while dialing dial tcp 192.168.49.2:2379: connect: connection refused". Reconnecting...
WARNING: 2022/02/09 18:55:07 [core] grpc: addrConn.createTransport failed to connect to {127.0.0.1:2379 127.0.0.1:2379 <nil> 0 <nil>}. Err: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused". Reconnecting...
{"level":"info","ts":"2022-02-09T18:55:07.949Z","caller":"etcdserver/server.go:1438","msg":"skipped leadership transfer for single voting member cluster","local-member-id":"aec36adc501070cc","current-leader-member-id":"aec36adc501070cc"}
{"level":"info","ts":"2022-02-09T18:55:08.039Z","caller":"embed/etcd.go:562","msg":"stopping serving peer traffic","address":"192.168.49.2:2380"}
{"level":"info","ts":"2022-02-09T18:55:08.041Z","caller":"embed/etcd.go:567","msg":"stopped serving peer traffic","address":"192.168.49.2:2380"}
{"level":"info","ts":"2022-02-09T18:55:08.042Z","caller":"embed/etcd.go:369","msg":"closed etcd server","name":"minikube","data-dir":"/var/lib/minikube/etcd","advertise-peer-urls":["https://192.168.49.2:2380"],"advertise-client-urls":["https://192.168.49.2:2379"]}

* 
* ==> kernel <==
*  18:33:49 up  7:44,  0 users,  load average: 0.67, 0.83, 1.10
Linux minikube 5.13.0-28-generic #31~20.04.1-Ubuntu SMP Wed Jan 19 14:08:10 UTC 2022 x86_64 x86_64 x86_64 GNU/Linux
PRETTY_NAME="Ubuntu 20.04.2 LTS"

* 
* ==> kube-apiserver [3229ea58fd90] <==
* W0209 18:55:16.436633       1 clientconn.go:1326] [core] grpc: addrConn.createTransport failed to connect to {127.0.0.1:2379 127.0.0.1 <nil> 0 <nil>}. Err: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused". Reconnecting...
W0209 18:55:16.438750       1 clientconn.go:1326] [core] grpc: addrConn.createTransport failed to connect to {127.0.0.1:2379 127.0.0.1 <nil> 0 <nil>}. Err: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused". Reconnecting...
W0209 18:55:16.455418       1 clientconn.go:1326] [core] grpc: addrConn.createTransport failed to connect to {127.0.0.1:2379 127.0.0.1 <nil> 0 <nil>}. Err: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused". Reconnecting...
W0209 18:55:16.490396       1 clientconn.go:1326] [core] grpc: addrConn.createTransport failed to connect to {127.0.0.1:2379 127.0.0.1 <nil> 0 <nil>}. Err: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused". Reconnecting...
W0209 18:55:16.502999       1 clientconn.go:1326] [core] grpc: addrConn.createTransport failed to connect to {127.0.0.1:2379 127.0.0.1 <nil> 0 <nil>}. Err: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused". Reconnecting...
W0209 18:55:16.512616       1 clientconn.go:1326] [core] grpc: addrConn.createTransport failed to connect to {127.0.0.1:2379 127.0.0.1 <nil> 0 <nil>}. Err: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused". Reconnecting...
W0209 18:55:16.518077       1 clientconn.go:1326] [core] grpc: addrConn.createTransport failed to connect to {127.0.0.1:2379 127.0.0.1 <nil> 0 <nil>}. Err: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused". Reconnecting...
W0209 18:55:16.526625       1 clientconn.go:1326] [core] grpc: addrConn.createTransport failed to connect to {127.0.0.1:2379 127.0.0.1 <nil> 0 <nil>}. Err: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused". Reconnecting...
W0209 18:55:16.565460       1 clientconn.go:1326] [core] grpc: addrConn.createTransport failed to connect to {127.0.0.1:2379 127.0.0.1 <nil> 0 <nil>}. Err: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused". Reconnecting...
W0209 18:55:16.582647       1 clientconn.go:1326] [core] grpc: addrConn.createTransport failed to connect to {127.0.0.1:2379 127.0.0.1 <nil> 0 <nil>}. Err: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused". Reconnecting...
W0209 18:55:16.595918       1 clientconn.go:1326] [core] grpc: addrConn.createTransport failed to connect to {127.0.0.1:2379 127.0.0.1 <nil> 0 <nil>}. Err: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused". Reconnecting...
W0209 18:55:16.616002       1 clientconn.go:1326] [core] grpc: addrConn.createTransport failed to connect to {127.0.0.1:2379 127.0.0.1 <nil> 0 <nil>}. Err: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused". Reconnecting...
W0209 18:55:16.629538       1 clientconn.go:1326] [core] grpc: addrConn.createTransport failed to connect to {127.0.0.1:2379 127.0.0.1 <nil> 0 <nil>}. Err: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused". Reconnecting...
W0209 18:55:16.669085       1 clientconn.go:1326] [core] grpc: addrConn.createTransport failed to connect to {127.0.0.1:2379 127.0.0.1 <nil> 0 <nil>}. Err: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused". Reconnecting...
W0209 18:55:16.671584       1 clientconn.go:1326] [core] grpc: addrConn.createTransport failed to connect to {127.0.0.1:2379 127.0.0.1 <nil> 0 <nil>}. Err: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused". Reconnecting...
W0209 18:55:16.684067       1 clientconn.go:1326] [core] grpc: addrConn.createTransport failed to connect to {127.0.0.1:2379 127.0.0.1 <nil> 0 <nil>}. Err: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused". Reconnecting...
W0209 18:55:16.703405       1 clientconn.go:1326] [core] grpc: addrConn.createTransport failed to connect to {127.0.0.1:2379 127.0.0.1 <nil> 0 <nil>}. Err: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused". Reconnecting...
W0209 18:55:16.710452       1 clientconn.go:1326] [core] grpc: addrConn.createTransport failed to connect to {127.0.0.1:2379 127.0.0.1 <nil> 0 <nil>}. Err: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused". Reconnecting...
W0209 18:55:16.722504       1 clientconn.go:1326] [core] grpc: addrConn.createTransport failed to connect to {127.0.0.1:2379 127.0.0.1 <nil> 0 <nil>}. Err: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused". Reconnecting...
W0209 18:55:16.723933       1 clientconn.go:1326] [core] grpc: addrConn.createTransport failed to connect to {127.0.0.1:2379 127.0.0.1 <nil> 0 <nil>}. Err: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused". Reconnecting...
W0209 18:55:16.756952       1 clientconn.go:1326] [core] grpc: addrConn.createTransport failed to connect to {127.0.0.1:2379 127.0.0.1 <nil> 0 <nil>}. Err: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused". Reconnecting...
W0209 18:55:16.780075       1 clientconn.go:1326] [core] grpc: addrConn.createTransport failed to connect to {127.0.0.1:2379 127.0.0.1 <nil> 0 <nil>}. Err: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused". Reconnecting...
W0209 18:55:16.780132       1 clientconn.go:1326] [core] grpc: addrConn.createTransport failed to connect to {127.0.0.1:2379 127.0.0.1 <nil> 0 <nil>}. Err: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused". Reconnecting...
W0209 18:55:16.823081       1 clientconn.go:1326] [core] grpc: addrConn.createTransport failed to connect to {127.0.0.1:2379 127.0.0.1 <nil> 0 <nil>}. Err: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused". Reconnecting...
W0209 18:55:16.859401       1 clientconn.go:1326] [core] grpc: addrConn.createTransport failed to connect to {127.0.0.1:2379 127.0.0.1 <nil> 0 <nil>}. Err: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused". Reconnecting...
W0209 18:55:16.893905       1 clientconn.go:1326] [core] grpc: addrConn.createTransport failed to connect to {127.0.0.1:2379 127.0.0.1 <nil> 0 <nil>}. Err: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused". Reconnecting...
W0209 18:55:16.896512       1 clientconn.go:1326] [core] grpc: addrConn.createTransport failed to connect to {127.0.0.1:2379 127.0.0.1 <nil> 0 <nil>}. Err: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused". Reconnecting...
W0209 18:55:17.002169       1 clientconn.go:1326] [core] grpc: addrConn.createTransport failed to connect to {127.0.0.1:2379 127.0.0.1 <nil> 0 <nil>}. Err: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused". Reconnecting...
W0209 18:55:17.030909       1 clientconn.go:1326] [core] grpc: addrConn.createTransport failed to connect to {127.0.0.1:2379 127.0.0.1 <nil> 0 <nil>}. Err: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused". Reconnecting...
W0209 18:55:17.056751       1 clientconn.go:1326] [core] grpc: addrConn.createTransport failed to connect to {127.0.0.1:2379 127.0.0.1 <nil> 0 <nil>}. Err: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused". Reconnecting...
W0209 18:55:17.067799       1 clientconn.go:1326] [core] grpc: addrConn.createTransport failed to connect to {127.0.0.1:2379 127.0.0.1 <nil> 0 <nil>}. Err: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused". Reconnecting...
W0209 18:55:17.087755       1 clientconn.go:1326] [core] grpc: addrConn.createTransport failed to connect to {127.0.0.1:2379 127.0.0.1 <nil> 0 <nil>}. Err: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused". Reconnecting...
W0209 18:55:17.095854       1 clientconn.go:1326] [core] grpc: addrConn.createTransport failed to connect to {127.0.0.1:2379 127.0.0.1 <nil> 0 <nil>}. Err: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused". Reconnecting...
W0209 18:55:17.101649       1 clientconn.go:1326] [core] grpc: addrConn.createTransport failed to connect to {127.0.0.1:2379 127.0.0.1 <nil> 0 <nil>}. Err: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused". Reconnecting...
W0209 18:55:17.109222       1 clientconn.go:1326] [core] grpc: addrConn.createTransport failed to connect to {127.0.0.1:2379 127.0.0.1 <nil> 0 <nil>}. Err: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused". Reconnecting...
W0209 18:55:17.122102       1 clientconn.go:1326] [core] grpc: addrConn.createTransport failed to connect to {127.0.0.1:2379 127.0.0.1 <nil> 0 <nil>}. Err: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused". Reconnecting...
W0209 18:55:17.141022       1 clientconn.go:1326] [core] grpc: addrConn.createTransport failed to connect to {127.0.0.1:2379 127.0.0.1 <nil> 0 <nil>}. Err: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused". Reconnecting...
W0209 18:55:17.141226       1 clientconn.go:1326] [core] grpc: addrConn.createTransport failed to connect to {127.0.0.1:2379 127.0.0.1 <nil> 0 <nil>}. Err: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused". Reconnecting...
W0209 18:55:17.214867       1 clientconn.go:1326] [core] grpc: addrConn.createTransport failed to connect to {127.0.0.1:2379 127.0.0.1 <nil> 0 <nil>}. Err: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused". Reconnecting...
W0209 18:55:17.222932       1 clientconn.go:1326] [core] grpc: addrConn.createTransport failed to connect to {127.0.0.1:2379 127.0.0.1 <nil> 0 <nil>}. Err: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused". Reconnecting...
W0209 18:55:17.238854       1 clientconn.go:1326] [core] grpc: addrConn.createTransport failed to connect to {127.0.0.1:2379 127.0.0.1 <nil> 0 <nil>}. Err: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused". Reconnecting...
W0209 18:55:17.248815       1 clientconn.go:1326] [core] grpc: addrConn.createTransport failed to connect to {127.0.0.1:2379 127.0.0.1 <nil> 0 <nil>}. Err: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused". Reconnecting...
W0209 18:55:17.270970       1 clientconn.go:1326] [core] grpc: addrConn.createTransport failed to connect to {127.0.0.1:2379 127.0.0.1 <nil> 0 <nil>}. Err: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused". Reconnecting...
W0209 18:55:17.276116       1 clientconn.go:1326] [core] grpc: addrConn.createTransport failed to connect to {127.0.0.1:2379 127.0.0.1 <nil> 0 <nil>}. Err: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused". Reconnecting...
W0209 18:55:17.276134       1 clientconn.go:1326] [core] grpc: addrConn.createTransport failed to connect to {127.0.0.1:2379 127.0.0.1 <nil> 0 <nil>}. Err: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused". Reconnecting...
W0209 18:55:17.280401       1 clientconn.go:1326] [core] grpc: addrConn.createTransport failed to connect to {127.0.0.1:2379 127.0.0.1 <nil> 0 <nil>}. Err: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused". Reconnecting...
W0209 18:55:17.281574       1 clientconn.go:1326] [core] grpc: addrConn.createTransport failed to connect to {127.0.0.1:2379 127.0.0.1 <nil> 0 <nil>}. Err: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused". Reconnecting...
W0209 18:55:17.307201       1 clientconn.go:1326] [core] grpc: addrConn.createTransport failed to connect to {127.0.0.1:2379 127.0.0.1 <nil> 0 <nil>}. Err: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused". Reconnecting...
W0209 18:55:17.307250       1 clientconn.go:1326] [core] grpc: addrConn.createTransport failed to connect to {127.0.0.1:2379 127.0.0.1 <nil> 0 <nil>}. Err: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused". Reconnecting...
W0209 18:55:17.520406       1 clientconn.go:1326] [core] grpc: addrConn.createTransport failed to connect to {127.0.0.1:2379 127.0.0.1 <nil> 0 <nil>}. Err: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused". Reconnecting...
W0209 18:55:17.577205       1 clientconn.go:1326] [core] grpc: addrConn.createTransport failed to connect to {127.0.0.1:2379 127.0.0.1 <nil> 0 <nil>}. Err: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused". Reconnecting...
W0209 18:55:17.631526       1 clientconn.go:1326] [core] grpc: addrConn.createTransport failed to connect to {127.0.0.1:2379 127.0.0.1 <nil> 0 <nil>}. Err: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused". Reconnecting...
W0209 18:55:17.631538       1 clientconn.go:1326] [core] grpc: addrConn.createTransport failed to connect to {127.0.0.1:2379 127.0.0.1 <nil> 0 <nil>}. Err: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused". Reconnecting...
W0209 18:55:17.652767       1 clientconn.go:1326] [core] grpc: addrConn.createTransport failed to connect to {127.0.0.1:2379 127.0.0.1 <nil> 0 <nil>}. Err: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused". Reconnecting...
W0209 18:55:17.719766       1 clientconn.go:1326] [core] grpc: addrConn.createTransport failed to connect to {127.0.0.1:2379 127.0.0.1 <nil> 0 <nil>}. Err: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused". Reconnecting...
W0209 18:55:17.899450       1 clientconn.go:1326] [core] grpc: addrConn.createTransport failed to connect to {127.0.0.1:2379 127.0.0.1 <nil> 0 <nil>}. Err: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused". Reconnecting...
W0209 18:55:17.907914       1 clientconn.go:1326] [core] grpc: addrConn.createTransport failed to connect to {127.0.0.1:2379 127.0.0.1 <nil> 0 <nil>}. Err: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused". Reconnecting...
W0209 18:55:17.942914       1 clientconn.go:1326] [core] grpc: addrConn.createTransport failed to connect to {127.0.0.1:2379 127.0.0.1 <nil> 0 <nil>}. Err: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused". Reconnecting...
W0209 18:55:18.030244       1 clientconn.go:1326] [core] grpc: addrConn.createTransport failed to connect to {127.0.0.1:2379 127.0.0.1 <nil> 0 <nil>}. Err: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused". Reconnecting...
W0209 18:55:18.169908       1 clientconn.go:1326] [core] grpc: addrConn.createTransport failed to connect to {127.0.0.1:2379 127.0.0.1 <nil> 0 <nil>}. Err: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused". Reconnecting...

* 
* ==> kube-apiserver [673547813dcc] <==
* W0211 18:10:32.929715       1 genericapiserver.go:455] Skipping API apps/v1beta2 because it has no resources.
W0211 18:10:32.929731       1 genericapiserver.go:455] Skipping API apps/v1beta1 because it has no resources.
W0211 18:10:32.931817       1 genericapiserver.go:455] Skipping API admissionregistration.k8s.io/v1beta1 because it has no resources.
I0211 18:10:32.936558       1 plugins.go:158] Loaded 12 mutating admission controller(s) successfully in the following order: NamespaceLifecycle,LimitRanger,ServiceAccount,NodeRestriction,TaintNodesByCondition,Priority,DefaultTolerationSeconds,DefaultStorageClass,StorageObjectInUseProtection,RuntimeClass,DefaultIngressClass,MutatingAdmissionWebhook.
I0211 18:10:32.936575       1 plugins.go:161] Loaded 11 validating admission controller(s) successfully in the following order: LimitRanger,ServiceAccount,PodSecurity,Priority,PersistentVolumeClaimResize,RuntimeClass,CertificateApproval,CertificateSigning,CertificateSubjectRestriction,ValidatingAdmissionWebhook,ResourceQuota.
W0211 18:10:32.955853       1 genericapiserver.go:455] Skipping API apiregistration.k8s.io/v1beta1 because it has no resources.
I0211 18:10:34.443093       1 dynamic_cafile_content.go:155] "Starting controller" name="request-header::/var/lib/minikube/certs/front-proxy-ca.crt"
I0211 18:10:34.443127       1 dynamic_cafile_content.go:155] "Starting controller" name="client-ca-bundle::/var/lib/minikube/certs/ca.crt"
I0211 18:10:34.443237       1 dynamic_serving_content.go:129] "Starting controller" name="serving-cert::/var/lib/minikube/certs/apiserver.crt::/var/lib/minikube/certs/apiserver.key"
I0211 18:10:34.443471       1 secure_serving.go:266] Serving securely on [::]:8443
I0211 18:10:34.443497       1 tlsconfig.go:240] "Starting DynamicServingCertificateController"
I0211 18:10:34.443677       1 apiservice_controller.go:97] Starting APIServiceRegistrationController
I0211 18:10:34.443689       1 cache.go:32] Waiting for caches to sync for APIServiceRegistrationController controller
I0211 18:10:34.443721       1 dynamic_serving_content.go:129] "Starting controller" name="aggregator-proxy-cert::/var/lib/minikube/certs/front-proxy-client.crt::/var/lib/minikube/certs/front-proxy-client.key"
I0211 18:10:34.443998       1 cluster_authentication_trust_controller.go:440] Starting cluster_authentication_trust_controller controller
I0211 18:10:34.444006       1 shared_informer.go:240] Waiting for caches to sync for cluster_authentication_trust_controller
I0211 18:10:34.444030       1 dynamic_cafile_content.go:155] "Starting controller" name="client-ca-bundle::/var/lib/minikube/certs/ca.crt"
I0211 18:10:34.444060       1 dynamic_cafile_content.go:155] "Starting controller" name="request-header::/var/lib/minikube/certs/front-proxy-ca.crt"
I0211 18:10:34.444118       1 available_controller.go:491] Starting AvailableConditionController
I0211 18:10:34.444128       1 cache.go:32] Waiting for caches to sync for AvailableConditionController controller
I0211 18:10:34.444145       1 controller.go:83] Starting OpenAPI AggregationController
I0211 18:10:34.444174       1 autoregister_controller.go:141] Starting autoregister controller
I0211 18:10:34.444180       1 cache.go:32] Waiting for caches to sync for autoregister controller
I0211 18:10:34.444195       1 apf_controller.go:312] Starting API Priority and Fairness config controller
I0211 18:10:34.444954       1 customresource_discovery_controller.go:209] Starting DiscoveryController
I0211 18:10:34.453312       1 controller.go:85] Starting OpenAPI controller
I0211 18:10:34.453413       1 naming_controller.go:291] Starting NamingConditionController
I0211 18:10:34.453482       1 establishing_controller.go:76] Starting EstablishingController
I0211 18:10:34.453583       1 nonstructuralschema_controller.go:192] Starting NonStructuralSchemaConditionController
I0211 18:10:34.453687       1 apiapproval_controller.go:186] Starting KubernetesAPIApprovalPolicyConformantConditionController
I0211 18:10:34.453828       1 crd_finalizer.go:266] Starting CRDFinalizer
I0211 18:10:34.460731       1 crdregistration_controller.go:111] Starting crd-autoregister controller
I0211 18:10:34.460934       1 shared_informer.go:240] Waiting for caches to sync for crd-autoregister
I0211 18:10:34.557219       1 apf_controller.go:317] Running API Priority and Fairness config worker
I0211 18:10:34.651999       1 shared_informer.go:247] Caches are synced for crd-autoregister 
I0211 18:10:34.657677       1 cache.go:39] Caches are synced for APIServiceRegistrationController controller
I0211 18:10:34.657946       1 cache.go:39] Caches are synced for AvailableConditionController controller
I0211 18:10:34.659674       1 shared_informer.go:247] Caches are synced for cluster_authentication_trust_controller 
I0211 18:10:34.662021       1 cache.go:39] Caches are synced for autoregister controller
E0211 18:10:34.665009       1 controller.go:152] Unable to remove old endpoints from kubernetes service: no master IPs were listed in storage, refusing to erase all endpoints for the kubernetes service
I0211 18:10:34.751358       1 shared_informer.go:247] Caches are synced for node_authorizer 
I0211 18:10:34.753715       1 controller.go:611] quota admission added evaluator for: leases.coordination.k8s.io
I0211 18:10:35.443872       1 controller.go:132] OpenAPI AggregationController: action for item : Nothing (removed from the queue).
I0211 18:10:35.443890       1 controller.go:132] OpenAPI AggregationController: action for item k8s_internal_local_delegation_chain_0000000000: Nothing (removed from the queue).
I0211 18:10:35.446842       1 storage_scheduling.go:148] all system priority classes are created successfully or already exist.
I0211 18:10:35.938524       1 controller.go:611] quota admission added evaluator for: serviceaccounts
I0211 18:10:35.950679       1 controller.go:611] quota admission added evaluator for: deployments.apps
I0211 18:10:35.974701       1 controller.go:611] quota admission added evaluator for: daemonsets.apps
I0211 18:10:35.983821       1 controller.go:611] quota admission added evaluator for: roles.rbac.authorization.k8s.io
I0211 18:10:35.987218       1 controller.go:611] quota admission added evaluator for: rolebindings.rbac.authorization.k8s.io
I0211 18:10:38.779463       1 controller.go:611] quota admission added evaluator for: events.events.k8s.io
I0211 18:10:46.765298       1 controller.go:611] quota admission added evaluator for: endpointslices.discovery.k8s.io
I0211 18:10:46.790354       1 controller.go:611] quota admission added evaluator for: endpoints
I0211 18:14:16.253750       1 controller.go:611] quota admission added evaluator for: namespaces
I0211 18:14:16.315361       1 controller.go:611] quota admission added evaluator for: replicasets.apps
I0211 18:14:16.463741       1 controller.go:611] quota admission added evaluator for: jobs.batch
I0211 18:14:56.730779       1 rest.go:387] Transition to non LoadBalancer type service or LoadBalancer type service with ExternalTrafficPolicy=Global
W0211 18:27:13.688473       1 watcher.go:229] watch chan error: etcdserver: mvcc: required revision has been compacted
I0211 18:30:23.707123       1 rest.go:379] Transition to LoadBalancer type service with ExternalTrafficPolicy=Local
I0211 18:31:42.612045       1 rest.go:387] Transition to non LoadBalancer type service or LoadBalancer type service with ExternalTrafficPolicy=Global

* 
* ==> kube-controller-manager [3c9c8b02aea2] <==
* I0211 18:10:46.763179       1 shared_informer.go:247] Caches are synced for HPA 
I0211 18:10:46.766171       1 shared_informer.go:247] Caches are synced for job 
I0211 18:10:46.767302       1 shared_informer.go:247] Caches are synced for deployment 
I0211 18:10:46.778588       1 shared_informer.go:247] Caches are synced for daemon sets 
I0211 18:10:46.778954       1 shared_informer.go:247] Caches are synced for disruption 
I0211 18:10:46.778968       1 disruption.go:371] Sending events to api server.
I0211 18:10:46.780609       1 shared_informer.go:247] Caches are synced for ephemeral 
I0211 18:10:46.780629       1 shared_informer.go:247] Caches are synced for endpoint 
I0211 18:10:46.782552       1 shared_informer.go:247] Caches are synced for ReplicaSet 
I0211 18:10:46.795671       1 event.go:291] "Event occurred" object="kube-system/kube-dns" kind="Endpoints" apiVersion="v1" type="Warning" reason="FailedToUpdateEndpoint" message="Failed to update endpoint kube-system/kube-dns: Operation cannot be fulfilled on endpoints \"kube-dns\": the object has been modified; please apply your changes to the latest version and try again"
I0211 18:10:46.803981       1 shared_informer.go:247] Caches are synced for resource quota 
I0211 18:10:46.851279       1 shared_informer.go:247] Caches are synced for resource quota 
I0211 18:10:46.868989       1 shared_informer.go:247] Caches are synced for taint 
I0211 18:10:46.869086       1 node_lifecycle_controller.go:1398] Initializing eviction metric for zone: 
I0211 18:10:46.869100       1 taint_manager.go:187] "Starting NoExecuteTaintManager"
W0211 18:10:46.869159       1 node_lifecycle_controller.go:1013] Missing timestamp for Node minikube. Assuming now as a timestamp.
I0211 18:10:46.869190       1 node_lifecycle_controller.go:1214] Controller detected that zone  is now in state Normal.
I0211 18:10:46.869202       1 event.go:291] "Event occurred" object="minikube" kind="Node" apiVersion="v1" type="Normal" reason="RegisteredNode" message="Node minikube event: Registered Node minikube in Controller"
I0211 18:10:47.259516       1 shared_informer.go:247] Caches are synced for garbage collector 
I0211 18:10:47.282571       1 shared_informer.go:247] Caches are synced for garbage collector 
I0211 18:10:47.282586       1 garbagecollector.go:151] Garbage collector: all resource monitors have synced. Proceeding to collect garbage
I0211 18:14:16.316669       1 event.go:291] "Event occurred" object="ingress-nginx/ingress-nginx-controller" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled up replica set ingress-nginx-controller-54d8b558d4 to 1"
I0211 18:14:16.354630       1 event.go:291] "Event occurred" object="ingress-nginx/ingress-nginx-controller-54d8b558d4" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: ingress-nginx-controller-54d8b558d4-5qjcm"
I0211 18:14:16.464976       1 job_controller.go:406] enqueueing job ingress-nginx/ingress-nginx-admission-create
I0211 18:14:16.470794       1 job_controller.go:406] enqueueing job ingress-nginx/ingress-nginx-admission-patch
I0211 18:14:16.473517       1 job_controller.go:406] enqueueing job ingress-nginx/ingress-nginx-admission-patch
I0211 18:14:16.473579       1 job_controller.go:406] enqueueing job ingress-nginx/ingress-nginx-admission-create
I0211 18:14:16.473594       1 event.go:291] "Event occurred" object="ingress-nginx/ingress-nginx-admission-patch" kind="Job" apiVersion="batch/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: ingress-nginx-admission-patch--1-8rxdp"
I0211 18:14:16.473609       1 event.go:291] "Event occurred" object="ingress-nginx/ingress-nginx-admission-create" kind="Job" apiVersion="batch/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: ingress-nginx-admission-create--1-kntkw"
I0211 18:14:16.476509       1 job_controller.go:406] enqueueing job ingress-nginx/ingress-nginx-admission-create
I0211 18:14:16.476547       1 job_controller.go:406] enqueueing job ingress-nginx/ingress-nginx-admission-patch
I0211 18:14:16.480526       1 job_controller.go:406] enqueueing job ingress-nginx/ingress-nginx-admission-create
I0211 18:14:16.480563       1 job_controller.go:406] enqueueing job ingress-nginx/ingress-nginx-admission-patch
I0211 18:14:16.491084       1 job_controller.go:406] enqueueing job ingress-nginx/ingress-nginx-admission-create
I0211 18:14:16.497027       1 job_controller.go:406] enqueueing job ingress-nginx/ingress-nginx-admission-patch
I0211 18:14:21.512812       1 job_controller.go:406] enqueueing job ingress-nginx/ingress-nginx-admission-patch
I0211 18:14:21.520421       1 job_controller.go:406] enqueueing job ingress-nginx/ingress-nginx-admission-create
I0211 18:14:21.520594       1 event.go:291] "Event occurred" object="ingress-nginx/ingress-nginx-admission-create" kind="Job" apiVersion="batch/v1" type="Normal" reason="Completed" message="Job completed"
I0211 18:14:21.529438       1 job_controller.go:406] enqueueing job ingress-nginx/ingress-nginx-admission-create
I0211 18:14:22.540658       1 job_controller.go:406] enqueueing job ingress-nginx/ingress-nginx-admission-patch
I0211 18:14:22.540813       1 event.go:291] "Event occurred" object="ingress-nginx/ingress-nginx-admission-patch" kind="Job" apiVersion="batch/v1" type="Normal" reason="Completed" message="Job completed"
I0211 18:14:22.547489       1 job_controller.go:406] enqueueing job ingress-nginx/ingress-nginx-admission-patch
I0211 18:14:23.558333       1 job_controller.go:406] enqueueing job ingress-nginx/ingress-nginx-admission-patch
I0211 18:14:56.732111       1 event.go:291] "Event occurred" object="ingress-nginx/ingress-nginx-controller" kind="Service" apiVersion="v1" type="Normal" reason="Type" message="LoadBalancer -> NodePort"
I0211 18:14:56.753287       1 event.go:291] "Event occurred" object="ingress-nginx/ingress-nginx-controller" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled up replica set ingress-nginx-controller-5f66978484 to 1"
I0211 18:14:56.758590       1 event.go:291] "Event occurred" object="ingress-nginx/ingress-nginx-controller-5f66978484" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: ingress-nginx-controller-5f66978484-nskp9"
I0211 18:15:16.788975       1 event.go:291] "Event occurred" object="ingress-nginx/ingress-nginx-controller" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled down replica set ingress-nginx-controller-54d8b558d4 to 0"
I0211 18:15:16.791674       1 event.go:291] "Event occurred" object="ingress-nginx/ingress-nginx-controller-54d8b558d4" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulDelete" message="Deleted pod: ingress-nginx-controller-54d8b558d4-5qjcm"
I0211 18:30:23.711560       1 event.go:291] "Event occurred" object="ingress-nginx/ingress-nginx-controller" kind="Service" apiVersion="v1" type="Normal" reason="Type" message="NodePort -> LoadBalancer"
I0211 18:30:23.758055       1 event.go:291] "Event occurred" object="ingress-nginx/ingress-nginx-controller" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled up replica set ingress-nginx-controller-54d8b558d4 to 1"
I0211 18:30:23.767059       1 event.go:291] "Event occurred" object="ingress-nginx/ingress-nginx-controller-54d8b558d4" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: ingress-nginx-controller-54d8b558d4-jvkkw"
I0211 18:30:43.796614       1 event.go:291] "Event occurred" object="ingress-nginx/ingress-nginx-controller" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled down replica set ingress-nginx-controller-5f66978484 to 0"
I0211 18:30:43.803241       1 event.go:291] "Event occurred" object="ingress-nginx/ingress-nginx-controller-5f66978484" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulDelete" message="Deleted pod: ingress-nginx-controller-5f66978484-nskp9"
I0211 18:31:16.734075       1 event.go:291] "Event occurred" object="default/demo" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled up replica set demo-654c477f6d to 1"
I0211 18:31:16.737638       1 event.go:291] "Event occurred" object="default/demo-654c477f6d" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: demo-654c477f6d-hrldp"
I0211 18:31:42.613393       1 event.go:291] "Event occurred" object="ingress-nginx/ingress-nginx-controller" kind="Service" apiVersion="v1" type="Normal" reason="Type" message="LoadBalancer -> NodePort"
I0211 18:31:42.668284       1 event.go:291] "Event occurred" object="ingress-nginx/ingress-nginx-controller" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled up replica set ingress-nginx-controller-5f66978484 to 1"
I0211 18:31:42.674425       1 event.go:291] "Event occurred" object="ingress-nginx/ingress-nginx-controller-5f66978484" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: ingress-nginx-controller-5f66978484-9c6zp"
I0211 18:32:02.775569       1 event.go:291] "Event occurred" object="ingress-nginx/ingress-nginx-controller" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled down replica set ingress-nginx-controller-54d8b558d4 to 0"
I0211 18:32:02.785272       1 event.go:291] "Event occurred" object="ingress-nginx/ingress-nginx-controller-54d8b558d4" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulDelete" message="Deleted pod: ingress-nginx-controller-54d8b558d4-jvkkw"

* 
* ==> kube-controller-manager [63b4e95072d7] <==
* I0209 18:21:12.347161       1 disruption.go:371] Sending events to api server.
I0209 18:21:12.348914       1 shared_informer.go:247] Caches are synced for namespace 
I0209 18:21:12.350728       1 shared_informer.go:247] Caches are synced for ephemeral 
I0209 18:21:12.351055       1 shared_informer.go:247] Caches are synced for endpoint 
I0209 18:21:12.352847       1 shared_informer.go:247] Caches are synced for HPA 
I0209 18:21:12.355048       1 shared_informer.go:247] Caches are synced for deployment 
I0209 18:21:12.356157       1 shared_informer.go:247] Caches are synced for stateful set 
I0209 18:21:12.358613       1 shared_informer.go:247] Caches are synced for taint 
I0209 18:21:12.358655       1 taint_manager.go:187] "Starting NoExecuteTaintManager"
I0209 18:21:12.358658       1 node_lifecycle_controller.go:1398] Initializing eviction metric for zone: 
I0209 18:21:12.358745       1 event.go:291] "Event occurred" object="minikube" kind="Node" apiVersion="v1" type="Normal" reason="RegisteredNode" message="Node minikube event: Registered Node minikube in Controller"
W0209 18:21:12.358749       1 node_lifecycle_controller.go:1013] Missing timestamp for Node minikube. Assuming now as a timestamp.
I0209 18:21:12.358776       1 node_lifecycle_controller.go:1214] Controller detected that zone  is now in state Normal.
I0209 18:21:12.360437       1 shared_informer.go:247] Caches are synced for PV protection 
I0209 18:21:12.362399       1 shared_informer.go:247] Caches are synced for GC 
I0209 18:21:12.364339       1 shared_informer.go:247] Caches are synced for job 
I0209 18:21:12.369094       1 shared_informer.go:247] Caches are synced for node 
I0209 18:21:12.369131       1 range_allocator.go:172] Starting range CIDR allocator
I0209 18:21:12.369135       1 shared_informer.go:240] Waiting for caches to sync for cidrallocator
I0209 18:21:12.369140       1 shared_informer.go:247] Caches are synced for cidrallocator 
I0209 18:21:12.372035       1 shared_informer.go:247] Caches are synced for ClusterRoleAggregator 
I0209 18:21:12.372150       1 shared_informer.go:247] Caches are synced for attach detach 
I0209 18:21:12.373201       1 shared_informer.go:247] Caches are synced for service account 
I0209 18:21:12.376519       1 shared_informer.go:247] Caches are synced for ReplicationController 
I0209 18:21:12.377598       1 shared_informer.go:247] Caches are synced for TTL 
I0209 18:21:12.378737       1 shared_informer.go:247] Caches are synced for TTL after finished 
I0209 18:21:12.380924       1 shared_informer.go:247] Caches are synced for daemon sets 
I0209 18:21:12.385871       1 shared_informer.go:247] Caches are synced for PVC protection 
I0209 18:21:12.484710       1 shared_informer.go:247] Caches are synced for cronjob 
I0209 18:21:12.547760       1 shared_informer.go:247] Caches are synced for resource quota 
I0209 18:21:12.551955       1 shared_informer.go:247] Caches are synced for endpoint_slice 
I0209 18:21:12.551974       1 shared_informer.go:247] Caches are synced for bootstrap_signer 
I0209 18:21:12.574246       1 shared_informer.go:247] Caches are synced for crt configmap 
I0209 18:21:12.579638       1 shared_informer.go:247] Caches are synced for endpoint_slice_mirroring 
I0209 18:21:12.587003       1 shared_informer.go:247] Caches are synced for resource quota 
I0209 18:21:13.002953       1 shared_informer.go:247] Caches are synced for garbage collector 
I0209 18:21:13.076407       1 shared_informer.go:247] Caches are synced for garbage collector 
I0209 18:21:13.076429       1 garbagecollector.go:151] Garbage collector: all resource monitors have synced. Proceeding to collect garbage
I0209 18:49:50.693316       1 event.go:291] "Event occurred" object="default/server-deployment" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled up replica set server-deployment-58cf857974 to 1"
I0209 18:49:50.695241       1 event.go:291] "Event occurred" object="default/worker-deployment" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled up replica set worker-deployment-7c94ff9b64 to 1"
I0209 18:49:50.701399       1 event.go:291] "Event occurred" object="default/server-deployment-58cf857974" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: server-deployment-58cf857974-56vdn"
I0209 18:49:50.701567       1 event.go:291] "Event occurred" object="default/worker-deployment-7c94ff9b64" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: worker-deployment-7c94ff9b64-z978x"
I0209 18:49:50.701571       1 event.go:291] "Event occurred" object="default/postgres-deployment" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled down replica set postgres-deployment-6796b9c68d to 0"
I0209 18:49:50.706178       1 event.go:291] "Event occurred" object="default/postgres-deployment-6796b9c68d" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulDelete" message="Deleted pod: postgres-deployment-6796b9c68d-7qfgd"
I0209 18:49:50.709063       1 event.go:291] "Event occurred" object="default/postgres-deployment" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled up replica set postgres-deployment-5b7fdb4969 to 1"
I0209 18:49:50.713177       1 event.go:291] "Event occurred" object="default/postgres-deployment-5b7fdb4969" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: postgres-deployment-5b7fdb4969-hk6td"
I0209 18:49:53.787899       1 event.go:291] "Event occurred" object="default/server-deployment" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled down replica set server-deployment-6b5cc7c876 to 2"
I0209 18:49:53.791628       1 event.go:291] "Event occurred" object="default/server-deployment-6b5cc7c876" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulDelete" message="Deleted pod: server-deployment-6b5cc7c876-ks4v6"
I0209 18:49:53.796048       1 event.go:291] "Event occurred" object="default/server-deployment" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled up replica set server-deployment-58cf857974 to 2"
I0209 18:49:53.798987       1 event.go:291] "Event occurred" object="default/server-deployment-58cf857974" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: server-deployment-58cf857974-7zmmp"
I0209 18:49:54.803269       1 event.go:291] "Event occurred" object="default/worker-deployment" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled down replica set worker-deployment-666c96ffc5 to 0"
I0209 18:49:54.807164       1 event.go:291] "Event occurred" object="default/worker-deployment-666c96ffc5" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulDelete" message="Deleted pod: worker-deployment-666c96ffc5-wdv98"
I0209 18:49:56.723673       1 event.go:291] "Event occurred" object="default/postgres-deployment" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled down replica set postgres-deployment-58b6c578d7 to 0"
I0209 18:49:56.727482       1 event.go:291] "Event occurred" object="default/postgres-deployment-58b6c578d7" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulDelete" message="Deleted pod: postgres-deployment-58b6c578d7-xbd9v"
I0209 18:49:58.523452       1 event.go:291] "Event occurred" object="default/server-deployment" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled down replica set server-deployment-6b5cc7c876 to 1"
I0209 18:49:58.527237       1 event.go:291] "Event occurred" object="default/server-deployment-6b5cc7c876" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulDelete" message="Deleted pod: server-deployment-6b5cc7c876-b7rpd"
I0209 18:49:58.531068       1 event.go:291] "Event occurred" object="default/server-deployment" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled up replica set server-deployment-58cf857974 to 3"
I0209 18:49:58.534615       1 event.go:291] "Event occurred" object="default/server-deployment-58cf857974" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: server-deployment-58cf857974-d2twr"
I0209 18:50:00.914997       1 event.go:291] "Event occurred" object="default/server-deployment" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled down replica set server-deployment-6b5cc7c876 to 0"
I0209 18:50:00.937952       1 event.go:291] "Event occurred" object="default/server-deployment-6b5cc7c876" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulDelete" message="Deleted pod: server-deployment-6b5cc7c876-mjcwd"

* 
* ==> kube-proxy [35f61d1bf74b] <==
* I0209 18:21:02.149655       1 node.go:172] Successfully retrieved node IP: 192.168.49.2
I0209 18:21:02.149742       1 server_others.go:140] Detected node IP 192.168.49.2
W0209 18:21:02.149776       1 server_others.go:565] Unknown proxy mode "", assuming iptables proxy
I0209 18:21:02.341229       1 server_others.go:206] kube-proxy running in dual-stack mode, IPv4-primary
I0209 18:21:02.341341       1 server_others.go:212] Using iptables Proxier.
I0209 18:21:02.341389       1 server_others.go:219] creating dualStackProxier for iptables.
W0209 18:21:02.341435       1 server_others.go:495] detect-local-mode set to ClusterCIDR, but no IPv6 cluster CIDR defined, , defaulting to no-op detect-local for IPv6
I0209 18:21:02.343263       1 server.go:649] Version: v1.22.3
I0209 18:21:02.345310       1 config.go:315] Starting service config controller
I0209 18:21:02.345846       1 config.go:224] Starting endpoint slice config controller
I0209 18:21:02.345865       1 shared_informer.go:240] Waiting for caches to sync for endpoint slice config
I0209 18:21:02.346120       1 shared_informer.go:240] Waiting for caches to sync for service config
I0209 18:21:02.446463       1 shared_informer.go:247] Caches are synced for service config 
I0209 18:21:02.446517       1 shared_informer.go:247] Caches are synced for endpoint slice config 

* 
* ==> kube-proxy [9b57892145de] <==
* I0211 18:10:38.745716       1 node.go:172] Successfully retrieved node IP: 192.168.49.2
I0211 18:10:38.745781       1 server_others.go:140] Detected node IP 192.168.49.2
W0211 18:10:38.745793       1 server_others.go:565] Unknown proxy mode "", assuming iptables proxy
I0211 18:10:38.772786       1 server_others.go:206] kube-proxy running in dual-stack mode, IPv4-primary
I0211 18:10:38.772806       1 server_others.go:212] Using iptables Proxier.
I0211 18:10:38.772817       1 server_others.go:219] creating dualStackProxier for iptables.
W0211 18:10:38.772831       1 server_others.go:495] detect-local-mode set to ClusterCIDR, but no IPv6 cluster CIDR defined, , defaulting to no-op detect-local for IPv6
I0211 18:10:38.773857       1 server.go:649] Version: v1.22.3
I0211 18:10:38.775191       1 config.go:224] Starting endpoint slice config controller
I0211 18:10:38.775313       1 config.go:315] Starting service config controller
I0211 18:10:38.775422       1 shared_informer.go:240] Waiting for caches to sync for service config
I0211 18:10:38.775421       1 shared_informer.go:240] Waiting for caches to sync for endpoint slice config
I0211 18:10:38.876346       1 shared_informer.go:247] Caches are synced for service config 
I0211 18:10:38.876344       1 shared_informer.go:247] Caches are synced for endpoint slice config 

* 
* ==> kube-scheduler [0169b6e51edf] <==
* I0209 18:20:55.537648       1 serving.go:347] Generated self-signed cert in-memory
W0209 18:20:58.837254       1 requestheader_controller.go:193] Unable to get configmap/extension-apiserver-authentication in kube-system.  Usually fixed by 'kubectl create rolebinding -n kube-system ROLEBINDING_NAME --role=extension-apiserver-authentication-reader --serviceaccount=YOUR_NS:YOUR_SA'
W0209 18:20:58.837341       1 authentication.go:345] Error looking up in-cluster authentication configuration: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot get resource "configmaps" in API group "" in the namespace "kube-system"
W0209 18:20:58.837374       1 authentication.go:346] Continuing without authentication configuration. This may treat all requests as anonymous.
W0209 18:20:58.846630       1 authentication.go:347] To require authentication configuration lookup to succeed, set --authentication-tolerate-lookup-failure=false
I0209 18:20:58.957166       1 configmap_cafile_content.go:201] "Starting controller" name="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
I0209 18:20:58.957216       1 shared_informer.go:240] Waiting for caches to sync for client-ca::kube-system::extension-apiserver-authentication::client-ca-file
I0209 18:20:58.957470       1 secure_serving.go:200] Serving securely on 127.0.0.1:10259
I0209 18:20:58.957533       1 tlsconfig.go:240] "Starting DynamicServingCertificateController"
I0209 18:20:59.057828       1 shared_informer.go:247] Caches are synced for client-ca::kube-system::extension-apiserver-authentication::client-ca-file 
I0209 18:55:08.238644       1 configmap_cafile_content.go:222] "Shutting down controller" name="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
I0209 18:55:08.239313       1 secure_serving.go:311] Stopped listening on 127.0.0.1:10259
I0209 18:55:08.239503       1 tlsconfig.go:255] "Shutting down DynamicServingCertificateController"

* 
* ==> kube-scheduler [2bd544b0dbdd] <==
* I0211 18:10:31.583934       1 serving.go:347] Generated self-signed cert in-memory
W0211 18:10:34.561918       1 requestheader_controller.go:193] Unable to get configmap/extension-apiserver-authentication in kube-system.  Usually fixed by 'kubectl create rolebinding -n kube-system ROLEBINDING_NAME --role=extension-apiserver-authentication-reader --serviceaccount=YOUR_NS:YOUR_SA'
W0211 18:10:34.561986       1 authentication.go:345] Error looking up in-cluster authentication configuration: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot get resource "configmaps" in API group "" in the namespace "kube-system"
W0211 18:10:34.562027       1 authentication.go:346] Continuing without authentication configuration. This may treat all requests as anonymous.
W0211 18:10:34.562679       1 authentication.go:347] To require authentication configuration lookup to succeed, set --authentication-tolerate-lookup-failure=false
I0211 18:10:34.769306       1 secure_serving.go:200] Serving securely on 127.0.0.1:10259
I0211 18:10:34.769860       1 configmap_cafile_content.go:201] "Starting controller" name="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
I0211 18:10:34.769906       1 shared_informer.go:240] Waiting for caches to sync for client-ca::kube-system::extension-apiserver-authentication::client-ca-file
I0211 18:10:34.769947       1 tlsconfig.go:240] "Starting DynamicServingCertificateController"
I0211 18:10:34.870181       1 shared_informer.go:247] Caches are synced for client-ca::kube-system::extension-apiserver-authentication::client-ca-file 
E0211 18:10:34.951587       1 plugin.go:138] "getting namespace, assuming empty set of namespace labels" err="namespace \"default\" not found" namespace="default"
E0211 18:10:34.951681       1 plugin.go:138] "getting namespace, assuming empty set of namespace labels" err="namespace \"default\" not found" namespace="default"
E0211 18:10:34.952042       1 plugin.go:138] "getting namespace, assuming empty set of namespace labels" err="namespace \"default\" not found" namespace="default"
E0211 18:10:34.952229       1 plugin.go:138] "getting namespace, assuming empty set of namespace labels" err="namespace \"default\" not found" namespace="default"
E0211 18:10:34.952405       1 plugin.go:138] "getting namespace, assuming empty set of namespace labels" err="namespace \"kube-system\" not found" namespace="kube-system"
E0211 18:10:34.952442       1 plugin.go:138] "getting namespace, assuming empty set of namespace labels" err="namespace \"default\" not found" namespace="default"
E0211 18:10:34.952499       1 plugin.go:138] "getting namespace, assuming empty set of namespace labels" err="namespace \"default\" not found" namespace="default"
E0211 18:10:34.952533       1 plugin.go:138] "getting namespace, assuming empty set of namespace labels" err="namespace \"kube-system\" not found" namespace="kube-system"
E0211 18:10:34.952585       1 plugin.go:138] "getting namespace, assuming empty set of namespace labels" err="namespace \"default\" not found" namespace="default"
E0211 18:10:34.952852       1 plugin.go:138] "getting namespace, assuming empty set of namespace labels" err="namespace \"default\" not found" namespace="default"

* 
* ==> kubelet <==
* -- Logs begin at Fri 2022-02-11 18:10:17 UTC, end at Fri 2022-02-11 18:33:49 UTC. --
Feb 11 18:14:56 minikube kubelet[936]: I0211 18:14:56.806317     936 reconciler.go:224] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-9z4hj\" (UniqueName: \"kubernetes.io/projected/9116c3a4-7df6-4ecb-b110-541405fd223b-kube-api-access-9z4hj\") pod \"ingress-nginx-controller-5f66978484-nskp9\" (UID: \"9116c3a4-7df6-4ecb-b110-541405fd223b\") "
Feb 11 18:14:57 minikube kubelet[936]: I0211 18:14:57.436612     936 docker_sandbox.go:401] "Failed to read pod IP from plugin/docker" err="Couldn't find network status for ingress-nginx/ingress-nginx-controller-5f66978484-nskp9 through plugin: invalid network status for"
Feb 11 18:14:58 minikube kubelet[936]: I0211 18:14:58.048469     936 docker_sandbox.go:401] "Failed to read pod IP from plugin/docker" err="Couldn't find network status for ingress-nginx/ingress-nginx-controller-5f66978484-nskp9 through plugin: invalid network status for"
Feb 11 18:15:03 minikube kubelet[936]: I0211 18:15:03.112726     936 docker_sandbox.go:401] "Failed to read pod IP from plugin/docker" err="Couldn't find network status for ingress-nginx/ingress-nginx-controller-5f66978484-nskp9 through plugin: invalid network status for"
Feb 11 18:15:28 minikube kubelet[936]: I0211 18:15:28.434728     936 reconciler.go:196] "operationExecutor.UnmountVolume started for volume \"webhook-cert\" (UniqueName: \"kubernetes.io/secret/393a7f44-6798-4a1e-8447-551d037d4e83-webhook-cert\") pod \"393a7f44-6798-4a1e-8447-551d037d4e83\" (UID: \"393a7f44-6798-4a1e-8447-551d037d4e83\") "
Feb 11 18:15:28 minikube kubelet[936]: I0211 18:15:28.434770     936 reconciler.go:196] "operationExecutor.UnmountVolume started for volume \"kube-api-access-xsr4z\" (UniqueName: \"kubernetes.io/projected/393a7f44-6798-4a1e-8447-551d037d4e83-kube-api-access-xsr4z\") pod \"393a7f44-6798-4a1e-8447-551d037d4e83\" (UID: \"393a7f44-6798-4a1e-8447-551d037d4e83\") "
Feb 11 18:15:28 minikube kubelet[936]: I0211 18:15:28.436231     936 operation_generator.go:866] UnmountVolume.TearDown succeeded for volume "kubernetes.io/secret/393a7f44-6798-4a1e-8447-551d037d4e83-webhook-cert" (OuterVolumeSpecName: "webhook-cert") pod "393a7f44-6798-4a1e-8447-551d037d4e83" (UID: "393a7f44-6798-4a1e-8447-551d037d4e83"). InnerVolumeSpecName "webhook-cert". PluginName "kubernetes.io/secret", VolumeGidValue ""
Feb 11 18:15:28 minikube kubelet[936]: I0211 18:15:28.436281     936 operation_generator.go:866] UnmountVolume.TearDown succeeded for volume "kubernetes.io/projected/393a7f44-6798-4a1e-8447-551d037d4e83-kube-api-access-xsr4z" (OuterVolumeSpecName: "kube-api-access-xsr4z") pod "393a7f44-6798-4a1e-8447-551d037d4e83" (UID: "393a7f44-6798-4a1e-8447-551d037d4e83"). InnerVolumeSpecName "kube-api-access-xsr4z". PluginName "kubernetes.io/projected", VolumeGidValue ""
Feb 11 18:15:28 minikube kubelet[936]: I0211 18:15:28.500483     936 scope.go:110] "RemoveContainer" containerID="8bf1c16c7080149fc812967bd3b5dbeb4610240e39671e516bcd939ecd11bb4f"
Feb 11 18:15:28 minikube kubelet[936]: I0211 18:15:28.511772     936 scope.go:110] "RemoveContainer" containerID="8bf1c16c7080149fc812967bd3b5dbeb4610240e39671e516bcd939ecd11bb4f"
Feb 11 18:15:28 minikube kubelet[936]: E0211 18:15:28.512189     936 remote_runtime.go:334] "ContainerStatus from runtime service failed" err="rpc error: code = Unknown desc = Error: No such container: 8bf1c16c7080149fc812967bd3b5dbeb4610240e39671e516bcd939ecd11bb4f" containerID="8bf1c16c7080149fc812967bd3b5dbeb4610240e39671e516bcd939ecd11bb4f"
Feb 11 18:15:28 minikube kubelet[936]: I0211 18:15:28.512217     936 pod_container_deletor.go:52] "DeleteContainer returned error" containerID={Type:docker ID:8bf1c16c7080149fc812967bd3b5dbeb4610240e39671e516bcd939ecd11bb4f} err="failed to get container status \"8bf1c16c7080149fc812967bd3b5dbeb4610240e39671e516bcd939ecd11bb4f\": rpc error: code = Unknown desc = Error: No such container: 8bf1c16c7080149fc812967bd3b5dbeb4610240e39671e516bcd939ecd11bb4f"
Feb 11 18:15:28 minikube kubelet[936]: I0211 18:15:28.534927     936 reconciler.go:319] "Volume detached for volume \"kube-api-access-xsr4z\" (UniqueName: \"kubernetes.io/projected/393a7f44-6798-4a1e-8447-551d037d4e83-kube-api-access-xsr4z\") on node \"minikube\" DevicePath \"\""
Feb 11 18:15:28 minikube kubelet[936]: I0211 18:15:28.534953     936 reconciler.go:319] "Volume detached for volume \"webhook-cert\" (UniqueName: \"kubernetes.io/secret/393a7f44-6798-4a1e-8447-551d037d4e83-webhook-cert\") on node \"minikube\" DevicePath \"\""
Feb 11 18:15:29 minikube kubelet[936]: E0211 18:15:29.666803     936 remote_runtime.go:276] "StopContainer from runtime service failed" err="rpc error: code = Unknown desc = Error response from daemon: No such container: 8bf1c16c7080149fc812967bd3b5dbeb4610240e39671e516bcd939ecd11bb4f" containerID="8bf1c16c7080149fc812967bd3b5dbeb4610240e39671e516bcd939ecd11bb4f"
Feb 11 18:15:29 minikube kubelet[936]: E0211 18:15:29.666844     936 kuberuntime_container.go:719] "Container termination failed with gracePeriod" err="rpc error: code = Unknown desc = Error response from daemon: No such container: 8bf1c16c7080149fc812967bd3b5dbeb4610240e39671e516bcd939ecd11bb4f" pod="ingress-nginx/ingress-nginx-controller-54d8b558d4-5qjcm" podUID=393a7f44-6798-4a1e-8447-551d037d4e83 containerName="controller" containerID="docker://8bf1c16c7080149fc812967bd3b5dbeb4610240e39671e516bcd939ecd11bb4f" gracePeriod=1
Feb 11 18:15:29 minikube kubelet[936]: E0211 18:15:29.666863     936 kuberuntime_container.go:744] "Kill container failed" err="rpc error: code = Unknown desc = Error response from daemon: No such container: 8bf1c16c7080149fc812967bd3b5dbeb4610240e39671e516bcd939ecd11bb4f" pod="ingress-nginx/ingress-nginx-controller-54d8b558d4-5qjcm" podUID=393a7f44-6798-4a1e-8447-551d037d4e83 containerName="controller" containerID={Type:docker ID:8bf1c16c7080149fc812967bd3b5dbeb4610240e39671e516bcd939ecd11bb4f}
Feb 11 18:15:29 minikube kubelet[936]: E0211 18:15:29.667737     936 kubelet.go:1767] failed to "KillContainer" for "controller" with KillContainerError: "rpc error: code = Unknown desc = Error response from daemon: No such container: 8bf1c16c7080149fc812967bd3b5dbeb4610240e39671e516bcd939ecd11bb4f"
Feb 11 18:15:29 minikube kubelet[936]: E0211 18:15:29.667774     936 pod_workers.go:836] "Error syncing pod, skipping" err="failed to \"KillContainer\" for \"controller\" with KillContainerError: \"rpc error: code = Unknown desc = Error response from daemon: No such container: 8bf1c16c7080149fc812967bd3b5dbeb4610240e39671e516bcd939ecd11bb4f\"" pod="ingress-nginx/ingress-nginx-controller-54d8b558d4-5qjcm" podUID=393a7f44-6798-4a1e-8447-551d037d4e83
Feb 11 18:15:29 minikube kubelet[936]: I0211 18:15:29.673307     936 kubelet_volumes.go:160] "Cleaned up orphaned pod volumes dir" podUID=393a7f44-6798-4a1e-8447-551d037d4e83 path="/var/lib/kubelet/pods/393a7f44-6798-4a1e-8447-551d037d4e83/volumes"
Feb 11 18:30:23 minikube kubelet[936]: I0211 18:30:23.770926     936 topology_manager.go:200] "Topology Admit Handler"
Feb 11 18:30:23 minikube kubelet[936]: I0211 18:30:23.879294     936 reconciler.go:224] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-pjhgz\" (UniqueName: \"kubernetes.io/projected/a6d6bb30-ec05-4f63-ad52-3ed7d788efe3-kube-api-access-pjhgz\") pod \"ingress-nginx-controller-54d8b558d4-jvkkw\" (UID: \"a6d6bb30-ec05-4f63-ad52-3ed7d788efe3\") "
Feb 11 18:30:23 minikube kubelet[936]: I0211 18:30:23.879330     936 reconciler.go:224] "operationExecutor.VerifyControllerAttachedVolume started for volume \"webhook-cert\" (UniqueName: \"kubernetes.io/secret/a6d6bb30-ec05-4f63-ad52-3ed7d788efe3-webhook-cert\") pod \"ingress-nginx-controller-54d8b558d4-jvkkw\" (UID: \"a6d6bb30-ec05-4f63-ad52-3ed7d788efe3\") "
Feb 11 18:30:24 minikube kubelet[936]: I0211 18:30:24.368462     936 docker_sandbox.go:401] "Failed to read pod IP from plugin/docker" err="Couldn't find network status for ingress-nginx/ingress-nginx-controller-54d8b558d4-jvkkw through plugin: invalid network status for"
Feb 11 18:30:24 minikube kubelet[936]: I0211 18:30:24.468161     936 docker_sandbox.go:401] "Failed to read pod IP from plugin/docker" err="Couldn't find network status for ingress-nginx/ingress-nginx-controller-54d8b558d4-jvkkw through plugin: invalid network status for"
Feb 11 18:30:25 minikube kubelet[936]: I0211 18:30:25.545186     936 docker_sandbox.go:401] "Failed to read pod IP from plugin/docker" err="Couldn't find network status for ingress-nginx/ingress-nginx-controller-54d8b558d4-jvkkw through plugin: invalid network status for"
Feb 11 18:30:55 minikube kubelet[936]: I0211 18:30:55.399161     936 reconciler.go:196] "operationExecutor.UnmountVolume started for volume \"webhook-cert\" (UniqueName: \"kubernetes.io/secret/9116c3a4-7df6-4ecb-b110-541405fd223b-webhook-cert\") pod \"9116c3a4-7df6-4ecb-b110-541405fd223b\" (UID: \"9116c3a4-7df6-4ecb-b110-541405fd223b\") "
Feb 11 18:30:55 minikube kubelet[936]: I0211 18:30:55.399317     936 reconciler.go:196] "operationExecutor.UnmountVolume started for volume \"kube-api-access-9z4hj\" (UniqueName: \"kubernetes.io/projected/9116c3a4-7df6-4ecb-b110-541405fd223b-kube-api-access-9z4hj\") pod \"9116c3a4-7df6-4ecb-b110-541405fd223b\" (UID: \"9116c3a4-7df6-4ecb-b110-541405fd223b\") "
Feb 11 18:30:55 minikube kubelet[936]: I0211 18:30:55.403911     936 operation_generator.go:866] UnmountVolume.TearDown succeeded for volume "kubernetes.io/secret/9116c3a4-7df6-4ecb-b110-541405fd223b-webhook-cert" (OuterVolumeSpecName: "webhook-cert") pod "9116c3a4-7df6-4ecb-b110-541405fd223b" (UID: "9116c3a4-7df6-4ecb-b110-541405fd223b"). InnerVolumeSpecName "webhook-cert". PluginName "kubernetes.io/secret", VolumeGidValue ""
Feb 11 18:30:55 minikube kubelet[936]: I0211 18:30:55.404229     936 operation_generator.go:866] UnmountVolume.TearDown succeeded for volume "kubernetes.io/projected/9116c3a4-7df6-4ecb-b110-541405fd223b-kube-api-access-9z4hj" (OuterVolumeSpecName: "kube-api-access-9z4hj") pod "9116c3a4-7df6-4ecb-b110-541405fd223b" (UID: "9116c3a4-7df6-4ecb-b110-541405fd223b"). InnerVolumeSpecName "kube-api-access-9z4hj". PluginName "kubernetes.io/projected", VolumeGidValue ""
Feb 11 18:30:55 minikube kubelet[936]: I0211 18:30:55.500216     936 reconciler.go:319] "Volume detached for volume \"webhook-cert\" (UniqueName: \"kubernetes.io/secret/9116c3a4-7df6-4ecb-b110-541405fd223b-webhook-cert\") on node \"minikube\" DevicePath \"\""
Feb 11 18:30:55 minikube kubelet[936]: I0211 18:30:55.500304     936 reconciler.go:319] "Volume detached for volume \"kube-api-access-9z4hj\" (UniqueName: \"kubernetes.io/projected/9116c3a4-7df6-4ecb-b110-541405fd223b-kube-api-access-9z4hj\") on node \"minikube\" DevicePath \"\""
Feb 11 18:30:55 minikube kubelet[936]: I0211 18:30:55.992874     936 scope.go:110] "RemoveContainer" containerID="b1d3a4e277257cc68a630a5fd0baec47a5cd5604911d22efe6306a969fbf031b"
Feb 11 18:30:56 minikube kubelet[936]: I0211 18:30:56.003542     936 scope.go:110] "RemoveContainer" containerID="b1d3a4e277257cc68a630a5fd0baec47a5cd5604911d22efe6306a969fbf031b"
Feb 11 18:30:56 minikube kubelet[936]: E0211 18:30:56.003980     936 remote_runtime.go:334] "ContainerStatus from runtime service failed" err="rpc error: code = Unknown desc = Error: No such container: b1d3a4e277257cc68a630a5fd0baec47a5cd5604911d22efe6306a969fbf031b" containerID="b1d3a4e277257cc68a630a5fd0baec47a5cd5604911d22efe6306a969fbf031b"
Feb 11 18:30:56 minikube kubelet[936]: I0211 18:30:56.004007     936 pod_container_deletor.go:52] "DeleteContainer returned error" containerID={Type:docker ID:b1d3a4e277257cc68a630a5fd0baec47a5cd5604911d22efe6306a969fbf031b} err="failed to get container status \"b1d3a4e277257cc68a630a5fd0baec47a5cd5604911d22efe6306a969fbf031b\": rpc error: code = Unknown desc = Error: No such container: b1d3a4e277257cc68a630a5fd0baec47a5cd5604911d22efe6306a969fbf031b"
Feb 11 18:30:57 minikube kubelet[936]: I0211 18:30:57.688622     936 kubelet_volumes.go:160] "Cleaned up orphaned pod volumes dir" podUID=9116c3a4-7df6-4ecb-b110-541405fd223b path="/var/lib/kubelet/pods/9116c3a4-7df6-4ecb-b110-541405fd223b/volumes"
Feb 11 18:31:16 minikube kubelet[936]: I0211 18:31:16.740875     936 topology_manager.go:200] "Topology Admit Handler"
Feb 11 18:31:16 minikube kubelet[936]: I0211 18:31:16.840311     936 reconciler.go:224] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-zffpg\" (UniqueName: \"kubernetes.io/projected/50e44a5b-bcee-4a25-ac94-e6188594f911-kube-api-access-zffpg\") pod \"demo-654c477f6d-hrldp\" (UID: \"50e44a5b-bcee-4a25-ac94-e6188594f911\") "
Feb 11 18:31:17 minikube kubelet[936]: I0211 18:31:17.365655     936 docker_sandbox.go:401] "Failed to read pod IP from plugin/docker" err="Couldn't find network status for default/demo-654c477f6d-hrldp through plugin: invalid network status for"
Feb 11 18:31:17 minikube kubelet[936]: I0211 18:31:17.365768     936 pod_container_deletor.go:79] "Container not found in pod's containers" containerID="34df80308cd7450538f97e84050803884d9da6e81b301422e8448f3053388d26"
Feb 11 18:31:18 minikube kubelet[936]: I0211 18:31:18.385497     936 docker_sandbox.go:401] "Failed to read pod IP from plugin/docker" err="Couldn't find network status for default/demo-654c477f6d-hrldp through plugin: invalid network status for"
Feb 11 18:31:23 minikube kubelet[936]: I0211 18:31:23.447838     936 docker_sandbox.go:401] "Failed to read pod IP from plugin/docker" err="Couldn't find network status for default/demo-654c477f6d-hrldp through plugin: invalid network status for"
Feb 11 18:31:42 minikube kubelet[936]: I0211 18:31:42.756460     936 topology_manager.go:200] "Topology Admit Handler"
Feb 11 18:31:42 minikube kubelet[936]: I0211 18:31:42.953589     936 reconciler.go:224] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-k6gfw\" (UniqueName: \"kubernetes.io/projected/89f4a28a-0890-4435-a387-0bae868e59e0-kube-api-access-k6gfw\") pod \"ingress-nginx-controller-5f66978484-9c6zp\" (UID: \"89f4a28a-0890-4435-a387-0bae868e59e0\") "
Feb 11 18:31:42 minikube kubelet[936]: I0211 18:31:42.953761     936 reconciler.go:224] "operationExecutor.VerifyControllerAttachedVolume started for volume \"webhook-cert\" (UniqueName: \"kubernetes.io/secret/89f4a28a-0890-4435-a387-0bae868e59e0-webhook-cert\") pod \"ingress-nginx-controller-5f66978484-9c6zp\" (UID: \"89f4a28a-0890-4435-a387-0bae868e59e0\") "
Feb 11 18:31:43 minikube kubelet[936]: I0211 18:31:43.753619     936 docker_sandbox.go:401] "Failed to read pod IP from plugin/docker" err="Couldn't find network status for ingress-nginx/ingress-nginx-controller-5f66978484-9c6zp through plugin: invalid network status for"
Feb 11 18:31:43 minikube kubelet[936]: I0211 18:31:43.968233     936 docker_sandbox.go:401] "Failed to read pod IP from plugin/docker" err="Couldn't find network status for ingress-nginx/ingress-nginx-controller-5f66978484-9c6zp through plugin: invalid network status for"
Feb 11 18:32:14 minikube kubelet[936]: I0211 18:32:14.446513     936 scope.go:110] "RemoveContainer" containerID="ee41341e1abd98ade767efb3e70bafd39e183bd76a8c702113832724de8b5439"
Feb 11 18:32:14 minikube kubelet[936]: I0211 18:32:14.457929     936 scope.go:110] "RemoveContainer" containerID="ee41341e1abd98ade767efb3e70bafd39e183bd76a8c702113832724de8b5439"
Feb 11 18:32:14 minikube kubelet[936]: E0211 18:32:14.458375     936 remote_runtime.go:334] "ContainerStatus from runtime service failed" err="rpc error: code = Unknown desc = Error: No such container: ee41341e1abd98ade767efb3e70bafd39e183bd76a8c702113832724de8b5439" containerID="ee41341e1abd98ade767efb3e70bafd39e183bd76a8c702113832724de8b5439"
Feb 11 18:32:14 minikube kubelet[936]: I0211 18:32:14.458405     936 pod_container_deletor.go:52] "DeleteContainer returned error" containerID={Type:docker ID:ee41341e1abd98ade767efb3e70bafd39e183bd76a8c702113832724de8b5439} err="failed to get container status \"ee41341e1abd98ade767efb3e70bafd39e183bd76a8c702113832724de8b5439\": rpc error: code = Unknown desc = Error: No such container: ee41341e1abd98ade767efb3e70bafd39e183bd76a8c702113832724de8b5439"
Feb 11 18:32:14 minikube kubelet[936]: I0211 18:32:14.471766     936 reconciler.go:196] "operationExecutor.UnmountVolume started for volume \"webhook-cert\" (UniqueName: \"kubernetes.io/secret/a6d6bb30-ec05-4f63-ad52-3ed7d788efe3-webhook-cert\") pod \"a6d6bb30-ec05-4f63-ad52-3ed7d788efe3\" (UID: \"a6d6bb30-ec05-4f63-ad52-3ed7d788efe3\") "
Feb 11 18:32:14 minikube kubelet[936]: I0211 18:32:14.471829     936 reconciler.go:196] "operationExecutor.UnmountVolume started for volume \"kube-api-access-pjhgz\" (UniqueName: \"kubernetes.io/projected/a6d6bb30-ec05-4f63-ad52-3ed7d788efe3-kube-api-access-pjhgz\") pod \"a6d6bb30-ec05-4f63-ad52-3ed7d788efe3\" (UID: \"a6d6bb30-ec05-4f63-ad52-3ed7d788efe3\") "
Feb 11 18:32:14 minikube kubelet[936]: I0211 18:32:14.473205     936 operation_generator.go:866] UnmountVolume.TearDown succeeded for volume "kubernetes.io/secret/a6d6bb30-ec05-4f63-ad52-3ed7d788efe3-webhook-cert" (OuterVolumeSpecName: "webhook-cert") pod "a6d6bb30-ec05-4f63-ad52-3ed7d788efe3" (UID: "a6d6bb30-ec05-4f63-ad52-3ed7d788efe3"). InnerVolumeSpecName "webhook-cert". PluginName "kubernetes.io/secret", VolumeGidValue ""
Feb 11 18:32:14 minikube kubelet[936]: I0211 18:32:14.473304     936 operation_generator.go:866] UnmountVolume.TearDown succeeded for volume "kubernetes.io/projected/a6d6bb30-ec05-4f63-ad52-3ed7d788efe3-kube-api-access-pjhgz" (OuterVolumeSpecName: "kube-api-access-pjhgz") pod "a6d6bb30-ec05-4f63-ad52-3ed7d788efe3" (UID: "a6d6bb30-ec05-4f63-ad52-3ed7d788efe3"). InnerVolumeSpecName "kube-api-access-pjhgz". PluginName "kubernetes.io/projected", VolumeGidValue ""
Feb 11 18:32:14 minikube kubelet[936]: I0211 18:32:14.572295     936 reconciler.go:319] "Volume detached for volume \"kube-api-access-pjhgz\" (UniqueName: \"kubernetes.io/projected/a6d6bb30-ec05-4f63-ad52-3ed7d788efe3-kube-api-access-pjhgz\") on node \"minikube\" DevicePath \"\""
Feb 11 18:32:14 minikube kubelet[936]: I0211 18:32:14.572327     936 reconciler.go:319] "Volume detached for volume \"webhook-cert\" (UniqueName: \"kubernetes.io/secret/a6d6bb30-ec05-4f63-ad52-3ed7d788efe3-webhook-cert\") on node \"minikube\" DevicePath \"\""
Feb 11 18:32:15 minikube kubelet[936]: I0211 18:32:15.679680     936 kubelet_volumes.go:160] "Cleaned up orphaned pod volumes dir" podUID=a6d6bb30-ec05-4f63-ad52-3ed7d788efe3 path="/var/lib/kubelet/pods/a6d6bb30-ec05-4f63-ad52-3ed7d788efe3/volumes"
Feb 11 18:32:27 minikube kubelet[936]: E0211 18:32:27.701465     936 fsHandler.go:114] failed to collect filesystem stats - rootDiskErr: could not stat "/var/lib/docker/overlay2/33f0d3377504cbbac7a1ccdcfae5722719f26e3df53b2f48f05204f02aa7feb2/diff" to get inode usage: stat /var/lib/docker/overlay2/33f0d3377504cbbac7a1ccdcfae5722719f26e3df53b2f48f05204f02aa7feb2/diff: no such file or directory, extraDiskErr: could not stat "/var/lib/docker/containers/ee41341e1abd98ade767efb3e70bafd39e183bd76a8c702113832724de8b5439" to get inode usage: stat /var/lib/docker/containers/ee41341e1abd98ade767efb3e70bafd39e183bd76a8c702113832724de8b5439: no such file or directory

* 
* ==> storage-provisioner [1cac57b0ddb1] <==
* I0211 18:10:49.819043       1 storage_provisioner.go:116] Initializing the minikube storage provisioner...
I0211 18:10:49.827660       1 storage_provisioner.go:141] Storage provisioner initialized, now starting service!
I0211 18:10:49.828191       1 leaderelection.go:243] attempting to acquire leader lease kube-system/k8s.io-minikube-hostpath...
I0211 18:11:07.261910       1 leaderelection.go:253] successfully acquired lease kube-system/k8s.io-minikube-hostpath
I0211 18:11:07.262013       1 event.go:282] Event(v1.ObjectReference{Kind:"Endpoints", Namespace:"kube-system", Name:"k8s.io-minikube-hostpath", UID:"7789e130-537f-40a0-b880-a7340857266f", APIVersion:"v1", ResourceVersion:"105093", FieldPath:""}): type: 'Normal' reason: 'LeaderElection' minikube_059fd37c-d9e9-40dd-a24f-583fb7dd1891 became leader
I0211 18:11:07.262040       1 controller.go:835] Starting provisioner controller k8s.io/minikube-hostpath_minikube_059fd37c-d9e9-40dd-a24f-583fb7dd1891!
I0211 18:11:07.362944       1 controller.go:884] Started provisioner controller k8s.io/minikube-hostpath_minikube_059fd37c-d9e9-40dd-a24f-583fb7dd1891!

* 
* ==> storage-provisioner [af7b5d113b9a] <==
* I0211 18:10:36.908305       1 storage_provisioner.go:116] Initializing the minikube storage provisioner...
F0211 18:10:36.917314       1 main.go:39] error getting server version: Get "https://10.96.0.1:443/version?timeout=32s": dial tcp 10.96.0.1:443: connect: no route to host

